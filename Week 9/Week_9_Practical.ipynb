{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6383be93-a913-4451-afa8-5539530fadb1",
   "metadata": {},
   "source": [
    "# ENN583 Week 9 - Object Detection\n",
    "\n",
    "This week's practical will explore how you can use 3 different object detectors from PyTorch to test an image, comparing how these detectors perform in different challenging conditions.\n",
    "\n",
    "In particular, we'll compare:\n",
    "* [Faster R-CNN](https://arxiv.org/pdf/1506.01497.pdf) -- one of the classic object detectors, a two-stage anchor-box architecture\n",
    "* [RetinaNet](https://arxiv.org/pdf/1708.02002.pdf) -- a high performing one-stage anchor-box architecture\n",
    "* [FCOS](https://arxiv.org/pdf/1904.01355.pdf) -- a popular one-stage point-based architecture \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0d3bb2-13a0-4e4b-87bf-bbceeda05e10",
   "metadata": {},
   "source": [
    "## Step 1: Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775dd41b-fa2d-4256-8482-2311729ac53a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.io.image import read_image\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights, fcos_resnet50_fpn, FCOS_ResNet50_FPN_Weights, retinanet_resnet50_fpn_v2, RetinaNet_ResNet50_FPN_V2_Weights\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "import torchvision\n",
    "\n",
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from colormap import sample_colors\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6629deba-e79a-470c-8a7c-8dc9272eed33",
   "metadata": {},
   "source": [
    "## Step 2: Load the detectors from PyTorch\n",
    "\n",
    "PyTorch has a number of pre-trained models available, including object detectors trained on COCO! \n",
    "\n",
    "You can see these [here](https://pytorch.org/vision/main/models.html#object-detection-instance-segmentation-and-person-keypoint-detection).\n",
    "\n",
    "As mentioned, we will compare Faster R-CNN (v2), RetinaNet (v2), and FCOS. Compare the 'Box MAP' of each of these models as listed on the linked Pytorch page -- which model do you expect to perform the best? Also consider the model number of parameters, and GFLOPS -- is there a big difference?\n",
    "\n",
    "Below, we have built off the pytorch example code to visualise how to use an object detector. Note the key steps:\n",
    "1. Initialise the model, load the weights and set into 'eval' mode\n",
    "    * Note how we need to set a box_score_thresh -- this is the minimum confidence threshold for a detection to be valid. Let's start with this low, and we can always filter detections later\n",
    "2. Load the data transforms necessary -- these typically ensure the image is the correct size and is normalized\n",
    "3. Preprocess the data using the data transform -- pretty self-explanatory\n",
    "4. Test the data with the model\n",
    "\n",
    "We're printing the output from the object detector -- how is it describing detections as an output? Can you recognise the different elements of the detection? How is the bounding box parameterised?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6695ccf8-2f13-48bd-895a-5cec1768c947",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img = read_image(\"images/coco/000000017029.jpg\")\n",
    "\n",
    "# Step 1: Initialize model with the best available weights\n",
    "weights_frcnn = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\n",
    "frcnn = fasterrcnn_resnet50_fpn_v2(weights=weights_frcnn, box_score_thresh=0.2)\n",
    "frcnn.eval()\n",
    "\n",
    "# Step 2: Initialize the inference transforms\n",
    "preprocess = weights_frcnn.transforms()\n",
    "\n",
    "# Step 3: Apply inference preprocessing transforms\n",
    "batch = [preprocess(img)]\n",
    "\n",
    "# Step 4: Use the model and visualize the prediction\n",
    "prediction = frcnn(batch)[0]\n",
    "\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0578a842-7789-4b6c-81bf-196b5d3ab5ee",
   "metadata": {},
   "source": [
    "## Step 3: Visualise the predictions\n",
    "\n",
    "Our prediction is a dictionary with a 'boxes', 'labels' and 'scores' field. Each detection has an entry in each field. The boxes are described as [xmin, ymin, xmax, ymax], and the labels are a class index. Below I'm printing the COCO class list in its entirety, as well as the labels predicted in this image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1c5be6-4ccd-4b4d-92d7-4468639d60b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_categories = weights_frcnn.meta[\"categories\"]\n",
    "print(coco_categories)\n",
    "\n",
    "#for every detection, print the coco category that was predicted\n",
    "print('Detected categories:')\n",
    "for lbl in prediction['labels']:\n",
    "    print(coco_categories[lbl])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69040559-ef07-4109-a990-cb763a5d6205",
   "metadata": {},
   "source": [
    "Below, I've created a **draw_detections** function that you can use to visualise results from the Pytorch detectors. You need to provide the image, the model prediction, and the name of the detector. Read through the function to understand how it works, and then visualise the results from the last test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62faef42-bf53-4fb0-8f38-81cea5879b93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def draw_detections(img, prediction = None, detectorName = 'F-RCNN'):\n",
    "    if prediction != None:\n",
    "        #extract the labels and scores -- combine these into text that can be printed on the image\n",
    "        labels = [coco_categories[i] for i in prediction[\"labels\"]]\n",
    "        scores = [f'{int(100.*torch.round(s, decimals=2))}%' for s in prediction['scores']]\n",
    "        print_txt = [f'{labels[i]}: {scores[i]}' for i in range(len(prediction['scores']))]\n",
    "    \n",
    "        # picks colors that are visually distinct to draw on the image\n",
    "        color_list = sample_colors(len(scores), rgb = True)\n",
    "        color_list_int = [(int(c[0]), int(c[1]), int(c[2])) for c in color_list]\n",
    "    \n",
    "        #creates a tensor image with the bboxes drawn on the image\n",
    "        box = draw_bounding_boxes(img, boxes=prediction[\"boxes\"],\n",
    "                                  labels=print_txt,\n",
    "                                  colors=color_list_int,\n",
    "                                  width=6, font_size=50)\n",
    "    else:\n",
    "        #draw the raw image only\n",
    "        box = img\n",
    "    \n",
    "    #put the image in format compatible with matplotlib\n",
    "    box = box.numpy()\n",
    "    box = np.swapaxes(box, 0, 1)\n",
    "    box = np.swapaxes(box, 1, 2)\n",
    "    \n",
    "    #draw with matplotlib\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(box)\n",
    "    ax.set_title(detectorName)\n",
    "\n",
    "    if prediction != None:\n",
    "    #text is hard to read -- create a custom legend\n",
    "        custom_lines = []\n",
    "        for c in color_list_int:\n",
    "            c_plt = (c[0]/255, c[1]/255, c[2]/255)\n",
    "            custom_lines += [Line2D([0], [0], color = c_plt, lw = 4)]\n",
    "        fig.legend(custom_lines, print_txt, loc = 'outside right center')\n",
    "                            \n",
    "    plt.show()\n",
    "    \n",
    "draw_detections(img, prediction, 'Faster R-CNN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8503969b-4c37-44e1-8066-772f33a008da",
   "metadata": {},
   "source": [
    "## Step 3: Testing over a set of images and extracting performance.\n",
    "\n",
    "### Load in the ground-truth dog labels\n",
    "To test performance, we need some ground-truth labels. Below I'm loading in a file that has the bounding box annotated for each dog in the image that we are trying to detect.\n",
    "\n",
    "Examine the ground-truth file -- what is it's format?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a041982-feb7-450c-9bbe-c8566cd11690",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('dog_object_labels.json', 'r') as f:\n",
    "    gt_labels = json.load(f)\n",
    "\n",
    "print(gt_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d28348-f583-40db-8bf0-87dd9f10f20b",
   "metadata": {},
   "source": [
    "### Test over the images \n",
    "The cell below tests Faster R-CNN on all the images in the 'images/coco' folder. This is a selection of images of dogs, that also have other objects present too! 2 visualisations will be drawn -- first the ground-truth label for the dog in the image, and secondly the predictions from Faster R-CNN.\n",
    "\n",
    "Look at the results, then adapt the code progressively to do the following: \n",
    "\n",
    "### **Your turn: Find all the TP and FN for class dog:** \n",
    "1. Calculate the IoU between all predicted bounding boxes and the ground-truth bounding box.\n",
    "2. Check if any predicted boxes overlap the dog with IoU greater than 0.5 (you may find the pytorch [box_iou function](https://pytorch.org/vision/main/generated/torchvision.ops.box_iou.html) useful).\n",
    "3. If no, this is a FN (undetected object).\n",
    "4. If yes, check if any of the overlapping boxes have a class label that is 'dog'. If so, this is a TP (classified and localised dog).\n",
    "5. If no, it is a FN (undetected object).\n",
    "\n",
    "**How many TPs and how many FNs were there? Try setting the IoU threshold higher, to 0.9 -- does this change the TPs and FNs?**\n",
    "\n",
    "_Note: we cannot identify FP's (spurious detections of objects that are not present) as we do not have labels for all the COCO class objects in the image. Does this sound familiar (hint Project 2)?_ \n",
    "\n",
    "### **Your turn: Find the IoU of all TPs for class dog:** \n",
    "Change the IoU threshold for TPs back to a reasonable value, like 0.5. \n",
    "\n",
    "Some TPs are more useful than others! Can you collect the IoU of all TPs? To do this, you'll need to check if there are multiple possible TPs (this is rare, but can happen). To do this:\n",
    "1. Of all the IoUs that meet the score and IoU threshold, find the prediction with the maximum IoU\n",
    "2. Save this IoU in a list.\n",
    "\n",
    "### **Your turn: Adding a confidence threshold** \n",
    "\n",
    "We now need to pick a reasonable minimum confidence threshold that keeps the TP detections for dog, and throws away other potential false predictions. \n",
    "1. Collect the confidence scores of all TPs and select a threshold that will allow you to keep all TPs. It would probably be best to choose a somewhat conservative threshold (not the absolute lowest score of the TPs you have observed). Make sure to collect the score of the prediction with the highest IoU.\n",
    "2. Threshold predictions using this new confidence score.\n",
    "\n",
    "**How does this change the predictions that are drawn?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f8b10f-ec7e-481b-8f06-fd2cff6b64e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for every image in the coco folder\n",
    "for idx, file_name in enumerate(glob.glob('images/coco/*.jpg')):\n",
    "    #read in the image\n",
    "    img = read_image(file_name)\n",
    "    \n",
    "    #make a prediction\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        batch = [preprocess(img)]\n",
    "\n",
    "        prediction = frcnn(batch)[0]\n",
    "\n",
    "    #draw the ground truth\n",
    "    gt_bbox = gt_labels[file_name]\n",
    "    #coco category 18 = dog\n",
    "    ground_truth = {'scores': torch.Tensor([1]), 'labels': torch.Tensor([18]).int(), 'boxes': torch.Tensor([gt_bbox])}\n",
    "    draw_detections(img, ground_truth, 'Ground-truth')\n",
    "\n",
    "    #draw the prediction\n",
    "    draw_detections(img, prediction, 'Faster R-CNN')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2040736-f838-4705-9b1d-b518e714e82e",
   "metadata": {},
   "source": [
    "## Step 4: Compare to RetinaNet and FCOS\n",
    "\n",
    "In the cell below, I've loaded the 2 other models we want to compare to. \n",
    "\n",
    "### Your turn: Test RetinaNet and FCOS\n",
    "\n",
    "Follow the same process as above, but test with RetinaNet and FCOS.\n",
    "**Note: you will need to find the correct conf_thresh for each detector again.**\n",
    "Do you observe: \n",
    "- different numbers of TPs?\n",
    "- different localisation qualities (IoU)?\n",
    "- any differences in detector behaviour?\n",
    "\n",
    "**Consider: How would you use this information to decide which detector to use?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd77f422-d316-4af8-92de-5a69f0fb2315",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "weights_fcos = FCOS_ResNet50_FPN_Weights.DEFAULT\n",
    "fcos = fcos_resnet50_fpn(weights=weights_fcos, box_score_thresh=0.2)\n",
    "fcos.eval()\n",
    "fcos_preprocess = weights_fcos.transforms()\n",
    "\n",
    "weights_retinanet = RetinaNet_ResNet50_FPN_V2_Weights.DEFAULT\n",
    "retinanet = retinanet_resnet50_fpn_v2(weights=weights_retinanet, box_score_thresh=0.2)\n",
    "retinanet.eval()\n",
    "retinanet_preprocess = weights_retinanet.transforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3656f9fe-778b-49c6-8eac-76386fc406b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test RetinaNet in this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c315556-8089-4b51-8eed-82a8976fa313",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test FCOS in this cell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b06b1ca-2a1e-4cf8-804a-ed66bda3c322",
   "metadata": {},
   "source": [
    "\n",
    "## Step 5: Object Detection in the Wild!\n",
    "\n",
    "**Your turn:** Test the detectors over all images in the images/challenging folder. This contains images of dogs in weird viewpoints, unusual-looking examples, poor illumination, occlusions, cluttered scenes, small objects. Note that there are no ground-truth labels for these images, but you can still visualise predictions to visually identify TPs.\n",
    "\n",
    "**Consider:**\n",
    "- What happens? Does the detector still detect the object?\n",
    "- Has the confidence threshold required changed?\n",
    "- How might this be relevant for Project 2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e58f786-cf2d-414e-a262-874edd0c4b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, file_name in enumerate(glob.glob('images/challenging/*')):\n",
    "    #read in the image\n",
    "    img = read_image(file_name)\n",
    "\n",
    "    #test one of the models below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb71376-2220-4625-885a-1c81a6320507",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6e0d48-2843-4395-af0a-4ef78f6d7194",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
