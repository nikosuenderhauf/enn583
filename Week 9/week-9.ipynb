{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6383be93-a913-4451-afa8-5539530fadb1",
   "metadata": {},
   "source": [
    "# ENN583 Week 9 - Deep Learning for Image Retrieval\n",
    "\n",
    "This week's practical will explore 2 different approaches to place recognition that use deep learning:\n",
    "* AlexNet - a global descriptor from AlexNet when pre-trained on ImageNet\n",
    "* HDC-DELF - local descriptors extracted from a DELF network and combined with hyperdimensional computing to create a global descriptor. The DELF network is built off a ResNet50 backbone.\n",
    "\n",
    "To do these experiments, we will use the [VPR tutorial github](https://github.com/stschubert/VPR_Tutorial/tree/main) and adapt their demo script.\n",
    "\n",
    "## Step 1: Clone the Visual Place Recognition Tutorial Github\n",
    "Clone the [VPR tutorial github](https://github.com/stschubert/VPR_Tutorial/tree/main)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df96455b-e121-48a6-98d9-4f4180c21c3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/stschubert/VPR_Tutorial.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0d3bb2-13a0-4e4b-87bf-bbceeda05e10",
   "metadata": {},
   "source": [
    "## Step 2: Install and import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96bbb64-1725-487e-853c-aeecdb062315",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow\n",
    "!pip install tensorflow_hub\n",
    "\n",
    "import argparse\n",
    "import configparser\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from VPR_Tutorial.evaluation.metrics import createPR, recallAt100precision, recallAtK\n",
    "from VPR_Tutorial.evaluation import show_correct_and_wrong_matches\n",
    "from VPR_Tutorial.matching import matching\n",
    "from VPR_Tutorial.datasets.load_dataset import GardensPointDataset, StLuciaDataset, SFUDataset\n",
    "import numpy as np\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual, interactive\n",
    "from IPython.display import display\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from glob import glob\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b06b1ca-2a1e-4cf8-804a-ed66bda3c322",
   "metadata": {},
   "source": [
    "## Step 3: Load and visualise the datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bc0160-6735-420b-8032-d26945cceaf8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = GardensPointDataset('VPR_Tutorial/images/GardensPoint/')\n",
    "imgs_db, imgs_q, GThard, GTsoft = dataset.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a06873-a46c-4b53-b903-4917ff2fdfae",
   "metadata": {
    "tags": []
   },
   "source": [
    "The gardens point dataset has 200 images in both the database and query set. There is a 1-to-1 correspondence between the database and query set --  images with the same ID were recorded at same places, i.e., the i-th database image matches the i-th query image. You can see this below, in the ground-truth assignment matrix, where there is a ground-truth match between the database and query set along the diagonal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddcaf1c-604e-4402-b2fc-75e830b42dc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.imshow(GThard)\n",
    "plt.colorbar()\n",
    "plt.xlabel('Database Image Idx')\n",
    "plt.ylabel('Query Set Image Idx')\n",
    "plt.title('GT Hard Association for Gardens Point Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1036e141-6a4f-46a4-ad49-13cabf547329",
   "metadata": {},
   "source": [
    "Let's view some of the images in the database and query set to get a sense of the data, as well as the main challenge of this dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf49b00-b381-4695-a7b9-29db2758451a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "im_idxs = [0, 50, 100, 150, 199]\n",
    "\n",
    "fig, ax = plt.subplots(2, 5, figsize = (15, 4))\n",
    "for i, idx in enumerate(im_idxs):\n",
    "\n",
    "    ax[0, i].imshow(imgs_db[idx])\n",
    "    ax[1, i].imshow(imgs_q[idx])\n",
    "    \n",
    "    ax[0, i].set_xticks([]) #turns off axis ticks, which can be nicer for images\n",
    "    ax[1, i].set_xticks([])\n",
    "    ax[0, i].set_yticks([])\n",
    "    ax[1, i].set_yticks([])\n",
    "    \n",
    "    ax[0, i].set_title(f'Image {idx}')\n",
    "\n",
    "ax[0, 0].set_ylabel('Database', rotation = 0, labelpad = 50, fontsize = 15)\n",
    "ax[1, 0].set_ylabel('Query Set', rotation = 0, labelpad = 50, fontsize = 15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6dc1a1d-117b-4a6c-9c43-efa338f65794",
   "metadata": {},
   "source": [
    "Can you see what the change to visual conditions is that would make this place recognition task challenging?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a0d788-b349-41fc-bfe0-cd9c49fc5dd1",
   "metadata": {},
   "source": [
    "## Step 4: Load a feature extractor and compute feature descriptors\n",
    "\n",
    "The main source of information about image correspondences are image feature descriptors. Local descriptors like DELF provide information for multiple regions of interest but are computationally expensive to compare. Holistic image descriptors like AlexNet reduce the computational complexity, but often have slightly lower performance. Feature aggregation methods such as Bag of Words, VLAD, or HDC can be used to combine the local descriptors of an image in a single holistic descriptor vector.\n",
    "\n",
    "In the following, you can select either a holistic (AlexNet) or local image descriptor (HDC-DELF). Try AlexNet first, then come back and compare performance to HDC-DELF.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef0ab0a-bd6b-4f3e-995d-79ca7413f199",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Widget for selecting a descriptor\n",
    "def select_descriptor(Descriptor=['AlexNet','HDC-DELF']):\n",
    "    return Descriptor\n",
    "w = interactive(select_descriptor)\n",
    "display(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84348d94-8754-48a0-bd0d-f72c127dffa4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# select descriptor\n",
    "descriptor = w.result\n",
    "\n",
    "print(f'===== Compute {descriptor} descriptors')\n",
    "feature_extractor = None\n",
    "if descriptor == 'HDC-DELF':\n",
    "    from VPR_Tutorial.feature_extraction.feature_extractor_holistic import HDCDELF\n",
    "    sys.path.append(os.path.join(os.getcwd(), \"VPR_Tutorial\"))\n",
    "    feature_extractor = HDCDELF()\n",
    "elif descriptor == 'AlexNet':\n",
    "    from VPR_Tutorial.feature_extraction.feature_extractor_holistic import AlexNetConv3Extractor\n",
    "    feature_extractor = AlexNetConv3Extractor()\n",
    "\n",
    "print('===== Compute reference set descriptors')\n",
    "db_D_holistic = feature_extractor.compute_features(imgs_db)\n",
    "print('===== Compute query set descriptors')\n",
    "q_D_holistic = feature_extractor.compute_features(imgs_q)\n",
    "print('===== Completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a12668-91a4-405d-8e2f-09489819ef9f",
   "metadata": {},
   "source": [
    "## Step 5. Examine the descriptor comparison and similarity matrix S\n",
    "To compare database and query descriptors, we use the cosine similarity (e.g. computed by the inner product of the normalized descriptor vectors). We can use this to get a sense of the correspondences being made by our model.\n",
    "\n",
    "In the following, the dense similarity matrix S between all descriptor pairs is computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04c93e6-95a8-4f28-bade-e225e59ba518",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# compare all descriptors and compute similarity matrix S\n",
    "print('===== Compute cosine similarities S')\n",
    "\n",
    "# normalize descriptors and compute S-matrix\n",
    "db_D_holistic = db_D_holistic / np.linalg.norm(db_D_holistic , axis=1, keepdims=True)\n",
    "q_D_holistic = q_D_holistic / np.linalg.norm(q_D_holistic , axis=1, keepdims=True)\n",
    "S = np.matmul(db_D_holistic , q_D_holistic.transpose())\n",
    "\n",
    "# show similarity matrix S\n",
    "fig = plt.imshow(S)\n",
    "fig.axes.set_xlabel('Database Image Idx')\n",
    "fig.axes.set_ylabel('Query Set Image Idx')\n",
    "fig.axes.set_title('Similarity matrix S')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6966322f-7d09-4f18-9cc2-33f57105d1d5",
   "metadata": {},
   "source": [
    "For AlexNet -- there appears to be a fairly diagonal trend, with some spots where there appears to be no strong match (e.g. around Query image 80), and other spots where there are many strong matches (e.g. around Query image 175).\n",
    "\n",
    "**Your turn: What's going on here? Visualise the query and database image at index 80 (not a strong match). Then, visualise the query image at index 175, and the database images between 170 and 180 (all strong matches).**\n",
    "\n",
    "You should be able to adapt the code that we used earlier to visualise the dataset to achieve this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dadaba-61fe-4140-a093-7217e5fe7757",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Your code goes below\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d41d5f-e597-4f32-a0c0-38ff04ab02ef",
   "metadata": {},
   "source": [
    "Any hypotheses on why these particular queries either have low similarity with all of the database, or high similarity with multiple images from the database?\n",
    "\n",
    "Note that this is qualitative analysis, where we make informed guesses about what's going on with the model, but these may not be true."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04498b6b-9ca6-4a6b-ad6a-ced6b70e47dc",
   "metadata": {},
   "source": [
    "## Step 6. Assess Performance\n",
    "\n",
    "Now that we've examined our data and the results, we can use performance metrics to summarise performance on the dataset. This is useful for comparing methods.\n",
    "\n",
    "To evaluate the quality of a similarity matrix S, we can apply a series of decreasing thresholds $\\theta$ to match more and more image pairs. Combined with ground-truth information, each threshold leads to a different set of true positives, false positives, true negatives and false negatives, which then provides one point on the precision-recall curve.\n",
    "\n",
    "In the following, the precision-recall curve and the area under the precision-recall curve is computed and visualized for *multi-match VPR*, i.e. all matches between each query image and the database have to be identified.\n",
    "\n",
    "To understand more about these metrics, read [this tutorial paper](https://arxiv.org/pdf/2303.03281.pdf) (pg.10-11)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3087f2-9c08-4b10-b6a1-f09db3a0ed84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# precision-recall curve\n",
    "P, R = createPR(S, GThard, GTsoft, matching='multi', n_thresh=100)\n",
    "plt.figure()\n",
    "plt.plot(R, P)\n",
    "plt.xlim(0, 1), plt.ylim(0, 1.01)\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Result on GardensPoint day_right--night_right')\n",
    "plt.grid('on')\n",
    "plt.draw()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e0589b-c439-41ba-8e41-d81d43f97ed7",
   "metadata": {},
   "source": [
    "The graph shows the precision-recall curve. A curve closer to the upper right corner would represent better performance. Precision=1 means that no false positives (FP) were extracted. A recall=1 means that all same places were found.\n",
    "\n",
    "Below, we calculate the area under this curve as a summary metric. The AUPRC performance ranges between 0 and 1 (higher is better)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d46b93-4b3d-49a6-b644-a26bf792a950",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# area under precision-recall curve (AUPRC)\n",
    "AUPRC = np.trapz(P, R)\n",
    "print(f'\\n===== AUPRC: {AUPRC:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58da156-76b6-4adb-bf5b-1d941969b05d",
   "metadata": {},
   "source": [
    "For AlexNet global descriptor, I get AUPRC 0.195 -- this is not very good.\n",
    "\n",
    "## Step 7. Compare to another method\n",
    "\n",
    "**Your turn: re-run the above experiments, now with the HDC-DELF descriptors! Do you have a gut feeling on whether it will do better or worse?**\n",
    "\n",
    "You should be able to scroll back up and re-run cells after selecting the new descriptor.\n",
    "\n",
    "Once you do the comparison, try to make some observations beyond comparing the AUPRC. For example, do you observe any difference in the appearance of the similarity matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11525929-5c67-479d-b5bd-4d4cd1a02248",
   "metadata": {},
   "source": [
    "## Step 8. VPR in the Wild!\n",
    "\n",
    "Collect some of your own images from outside S block at QUT! You can upload these into the 'MyData' folder in the 'images' folder and test them below. I've put a few placeholders, but you should go take your own. Note: you should probably take images in landscape mode with your phone to match the dataset.\n",
    "\n",
    "**Your turn: The code below will load in your images, extract their features, but is missing the code to compute the similarity matrix! Add this in yourself, then you will be able to visualise the matches for each image.**\n",
    "\n",
    "How do the models perform?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34e3ecd-3a27-4f29-8be0-11c11f68e6a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " # load images\n",
    "my_fns_q = glob('MyData/*.jpg')\n",
    "my_imgs_q = [np.array(Image.open(fn)) for fn in my_fns_q]\n",
    "\n",
    "print('===== Compute query set descriptors')\n",
    "q_my_holistic = feature_extractor.compute_features(my_imgs_q)\n",
    "\n",
    "print('===== Normalize query set descriptors')\n",
    "q_my_holistic = q_my_holistic / np.linalg.norm(q_my_holistic , axis=1, keepdims=True)\n",
    "\n",
    "#### Your turn: Create the similarity matrix below\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f745d6-eb1e-4e09-ae2e-3a78956240b8",
   "metadata": {},
   "source": [
    "The similarity matrix is not very helpful to look at with these images, as we don't necessarily know the ground-truth matches. We can however visualise the matches below!\n",
    "\n",
    "Where the matches correct? If not, have a look at the similarity of the matches compared to the similarity matrix calculated with the normal query set -- do you think the matches were confident matches?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3045c48f-5a0b-496a-b7a6-3df5df720708",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "im_matches = np.argmax(S, axis = 0)\n",
    "\n",
    "for orig_idx in range(len(im_matches)):\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2, figsize = (6, 4))\n",
    "    \n",
    "    ax[0].imshow(imgs_db[im_matches[orig_idx]])\n",
    "    ax[1].imshow(my_imgs_q[orig_idx])\n",
    "    \n",
    "    ax[0].set_xticks([]) #turns off axis ticks, which can be nicer for images\n",
    "    ax[1].set_xticks([])\n",
    "    ax[0].set_yticks([])\n",
    "    ax[1].set_yticks([])\n",
    "\n",
    "    ax[0].set_title('Database', rotation = 0, fontsize = 15)\n",
    "    ax[1].set_title('Query Image', rotation = 0, fontsize = 15)\n",
    "    \n",
    "    similarity = np.max(S, axis = 0)[orig_idx]\n",
    "    fig.suptitle(f'Similarity of match: {similarity}', y = 0.85, fontsize = 15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dc6dac-4f72-4be1-8d02-f6cb47372a13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
