{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENN583 - Week 4 Prac - Estimating Camera Motion \n",
    "\n",
    "In this Prac notebook, you can experiment with the three different ways of estimating the camera motion based on feature correspondences.\n",
    "\n",
    "Using the familiar sequence from the Kitti dataset, the notebook shows you how to implement the 2D-2D method based on the Essential Matrix. You can then refine this method by determining the absolute scale, which is typically missing from a purely monocular visual odometry system, but can be recovered here from the stereo information.\n",
    "\n",
    "The later parts of this notebook let you investigate the 3D-3D method based on Arun's algorithm. An implementation is provided.\n",
    "Finally, you can implement a 3D-2D correspondence pipeline and use the `solvePnP()` method to determine the camera motion between two frames.\n",
    "\n",
    "### Setting Things Up\n",
    "As always, we start by setting up the Kitti dataset object and import all required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if pykitti is installed, if not use pip to set it up\n",
    "try:\n",
    "    import pykitti\n",
    "except ImportError:\n",
    "    !pip install pykitti\n",
    "    import pykitti\n",
    "\n",
    "# Read the dataset sequence we just downloaded\n",
    "basedir = '../kitti'\n",
    "date = '2011_09_26'\n",
    "drive = '0035'\n",
    "\n",
    "# The 'frames' argument is optional - default: None, which loads the whole dataset.\n",
    "# data = pykitti.raw(basedir, date, drive, frames=range(0, 50, 5))\n",
    "data = pykitti.raw(basedir, date, drive)\n",
    "\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from spatialmath import *\n",
    "from spatialmath.base import *\n",
    "from spatialmath.base import sym\n",
    "from spatialgeometry import *\n",
    "\n",
    "# get the left and right stereo images from the 10th frame\n",
    "left, right = data.get_rgb(10)\n",
    "left = np.array(left)\n",
    "right = np.array(right)\n",
    "\n",
    "# use plt to show them side by side\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(left, cmap='gray')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(right, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convenience Functions \n",
    "Here we define convenience functions for feature extraction and matching. Hint: If you want, you can implement different feature detectors / descriptors or matching algorithms here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_features(img, method='sift'):\n",
    "    \"\"\"Detect features in an image.\"\"\"\n",
    "    \n",
    "    if method == 'sift':\n",
    "        # detect features\n",
    "        sift = cv2.xfeatures2d.SIFT_create()\n",
    "        kp, des = sift.detectAndCompute(img, None)\n",
    "    elif method == 'orb':\n",
    "        # detect features\n",
    "        orb = cv2.ORB_create()\n",
    "        kp, des = orb.detectAndCompute(img, None)\n",
    "    else:\n",
    "        # not implemented error\n",
    "        raise NotImplementedError('Unknown feature detection method.')\n",
    "        \n",
    "    return kp, des\n",
    "\n",
    "def match_features(des1, des2):\n",
    "    \"\"\"Match features between two images.\"\"\"\n",
    "    \n",
    "    # match features\n",
    "    bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n",
    "    matches = bf.match(des1, des2)\n",
    "    \n",
    "    return matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motion from 2D-2D Correspondences\n",
    "\n",
    "Here we use OpenCV's inbuilt functions to estimate the camera motion from 2D-2D correspondences. This example shows how to use the `findEssentialMat` and `recoverPose` functions. \n",
    "\n",
    "We expect two input images in grayscale format, along with the camera matrix K. The function returns a SE3 object from the spatial math toolbox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def motion_from_2D2D(img1, img2, K):\n",
    "    \"\"\"Estimate the motion between two images using the 2D-2D method.\"\"\"\n",
    "    \n",
    "    # detect features in both images\n",
    "    kp1, des1 = detect_features(img1)\n",
    "    kp2, des2 = detect_features(img2)\n",
    "\n",
    "    # match features between the two images\n",
    "    matches = match_features(des1, des2)\n",
    "\n",
    "    # estimate the essential matrix\n",
    "    E, mask = cv2.findEssentialMat(\n",
    "        np.array([kp1[m.queryIdx].pt for m in matches]),\n",
    "        np.array([kp2[m.trainIdx].pt for m in matches]),\n",
    "        cameraMatrix=K, method=cv2.RANSAC, prob=0.999, threshold=1.0)\n",
    "    \n",
    "    # recover the pose from the essential matrix\n",
    "    points, R, t, mask = cv2.recoverPose(E,\n",
    "        np.array([kp1[m.queryIdx].pt for m in matches]),\n",
    "        np.array([kp2[m.trainIdx].pt for m in matches]))\n",
    "    \n",
    "    # we return a SE3 object describing the pose of the second camera in the frame of the first camera\n",
    "    # we also return the mask of inliers\n",
    "    return SE3(t) * SE3(SO3(R)), mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the function above to estimate the camera motion along a whole trajectory. This will take around 30 seconds for the example trajectory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will store the estimated frame-to-frame motion in this list\n",
    "# this will contain SE3 objects describing the _relative_ motion of the camera from one frame to the next\n",
    "relative_poses = []\n",
    "relative_poses.append(SE3())    # put the first camera at the origin\n",
    "\n",
    "# we will store the mask of inliers in this list\n",
    "masks = []\n",
    "\n",
    "# the intrinsic parameters of the left RGB camera\n",
    "K = data.calib.K_cam2\n",
    "\n",
    "# we iterate over all RGB images in the dataset\n",
    "img1 = None\n",
    "\n",
    "for left, right_ in data.rgb:\n",
    "\n",
    "    # convert the image into opencv format and grayscale\n",
    "    left = cv2.cvtColor(np.array(left), cv2.COLOR_RGB2GRAY)    \n",
    "\n",
    "    # is this the first iteration?\n",
    "    if img1 is None:\n",
    "        img1 = left       \n",
    "        continue\n",
    "    else:\n",
    "        # now img2 is the most recent image, img1 is the previous image\n",
    "        img2 = left\n",
    "\n",
    "        # estimate the motion between two cameras, returns a SE3 object\n",
    "        # the SE3 is the pose of the second camera in the frame of the first camera\n",
    "        T, mask = motion_from_2D2D(img1, img2, K)\n",
    "        \n",
    "        relative_poses.append(T)\n",
    "        masks.append(mask)\n",
    "\n",
    "        # img2 becomes the previous image for the next iteration\n",
    "        img1 = img2    \n",
    "\n",
    "        if len(relative_poses) % 25 == 0:\n",
    "            print(\"Processed %d images\" % len(relative_poses))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the relative motion between the adjecent camera frames stored in the `relative_poses` array.\n",
    "\n",
    "How can we compose a trajectory in world frame from this? Remember that the relative motion will always be relative to the camera frames. \n",
    "\n",
    "Also remember that the 2D2D method cannot give us the absolute scale of the translation, only the direction. We are going to cheat a little bit here and determine the absolute scale factor from the ground truth motion. \n",
    "\n",
    "**Your turn:**\n",
    " - Replace the cheating and find another way of establishing the scale factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Let's establish the position of the first camera in the world frame.\n",
    "# We will assume that the first camera is looking along the x-axis of the world frame and is located at (0,0,0)\n",
    "c1Rw = np.array([[0,-1,0],[0,0,-1],[1,0,0]])\n",
    "c1Tw = SE3(SO3(c1Rw)) \n",
    "wTc1 = c1Tw.inv()       # this is the pose of camera 1 in the world frame, as seen by the world frame\n",
    "\n",
    "# we will store the poses of the cameras in the world frame in this list\n",
    "# initialise it with the pose of the first camera\n",
    "trajectory = [wTc1]\n",
    "\n",
    "# let's iterate over the relative poses and compute the poses in the world frame\n",
    "\n",
    "for i, c2Tc1 in enumerate(relative_poses[1:]):     \n",
    "    \n",
    "    # recoverPose gives us the c2Tc1 transform, but we want the c1Tc2 transform. c1Tc2 gives us the pose of camera 2 in the frame of camera 1.\n",
    "    c1Tc2 = c2Tc1.inv() \n",
    "\n",
    "    # cheat by getting the scale factor from the ground truth pose data\n",
    "    d = SE3(data.oxts[i+1].T_w_imu).inv() * SE3(data.oxts[i].T_w_imu)   \n",
    "    scale = np.linalg.norm(d.t)       \n",
    "\n",
    "    # ========\n",
    "    # YOUR TURN! Find a way to compute the scale factor without using the ground truth data. Remember you have access to a stereo camera.\n",
    "    # YOUR CODE HERE\n",
    "    # scale = ...\n",
    "    # ========\n",
    "\n",
    "    # scale the translation vector\n",
    "    c1Tc2.t *= scale\n",
    "\n",
    "    # if the camera is moving backwards (-z direction) we need to invert the relative pose\n",
    "    # this should have been caught by OpenCV's recoverPose function (?)\n",
    "    if c1Tc2.t[2] < 0:\n",
    "        c1Tc2.t[2] *= -1\n",
    "    \n",
    "    # this is the pose of the current camera in the world frame\n",
    "    trajectory.append(trajectory[-1] * c1Tc2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ===== PLOTTING =====\n",
    "\n",
    "# the world frame is not aligned with the ground truth frame (we'll call it g), so let's \n",
    "# get the first ground truth pose and use that to align world and ground truth frames\n",
    "gTw = SE3(data.oxts[0].T_w_imu)\n",
    "\n",
    "# our trajectory now is in world frame, so let's transform it to the ground truth frame\n",
    "trajectory = [gTw * T for T in trajectory]\n",
    "\n",
    "# plot the estimated trajectory\n",
    "traj = np.array([ [T.t[0] for T in trajectory], [T.t[1] for T in trajectory] ])\n",
    "plt.plot(traj[0,:], traj[1,:],'b-', label='Estimated Trajectory')\n",
    "\n",
    "# plot the ground truth into the same plot\n",
    "gt = np.array([ [oxts.T_w_imu[0,3] for oxts in data.oxts], [oxts.T_w_imu[1,3] for oxts in data.oxts] ])\n",
    "plt.plot(gt[0,:], gt[1,:],'r-', label='Ground Truth')\n",
    "#  give the plot an aspect ratio so that both axes are scaled equally\n",
    "plt.axis('equal')\n",
    "plt.xlabel('x'); plt.ylabel('y'); plt.title('Trajectory')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "# we can also plot more information, such as the error between the estimated and ground truth trajectories\n",
    "# we also plot information about the number of matches and inliners, returned in the `masks` array we got in the cell above\n",
    "error = np.linalg.norm(traj - gt, axis=0)\n",
    "plt.figure()\n",
    "plt.plot(error*1000,'r', label='ATE [mm]')\n",
    "plt.plot([len(m) for m in masks], label='Matched Features')\n",
    "plt.plot([np.sum(m)/255 for m in masks], '--', label='Inliers')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "# print some error metrics\n",
    "# ATE = Absolute Pose Error (but we only calculate it on the XY plane here)\n",
    "# RMSE = Root Mean Squared Error\n",
    "print(f'Mean ATE: \\t{np.mean(error)*1000:.2f} mm')\n",
    "print(f'Median ATE: \\t{np.median(error)*1000:.2f} mm')\n",
    "print(f'Max ATE: \\t{np.max(error)*1000:.2f} mm')\n",
    "print(f'RMSE: \\t\\t{np.sqrt(np.mean((traj - gt)**2))*1000:.2f} mm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motion from 3D-3D correspondences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def motion_from_3D3D(X,Y):\n",
    "    \"\"\"Implements Arun's method for computing the motion from two sets of 3D points.    \n",
    "    This returns a transform c2Tc1 such that Y = c2Tc1 * X, where X are the points before motion (first camera) \n",
    "    and Y are points after motion (second camera). The method is described in Arun et al. (1987)    \n",
    "    \"\"\"\n",
    "\n",
    "    # determine the centroid\n",
    "    mean_X = np.mean(X, axis=1)\n",
    "    mean_Y = np.mean(Y, axis=1)\n",
    "\n",
    "    # get the centered coordinates by subtracting the centroid\n",
    "    XX = X - mean_X.reshape(-1,1)\n",
    "    YY = Y - mean_Y.reshape(-1,1)\n",
    "\n",
    "    # compute the 3x3vcovariance matrix\n",
    "    H = np.dot(XX,YY.T)\n",
    "\n",
    "    # compute the SVD of the covariance matrix\n",
    "    U, S, Vt = np.linalg.svd(H)\n",
    "\n",
    "    # compute the rotation matrix from U and Vt\n",
    "    R = np.dot(Vt.T, U.T)\n",
    "    \n",
    "    # check if the R is a reflection and correct if needed\n",
    "    if np.linalg.det(R)<0:  # det(R) is 1 for a rotation and -1 for a reflection\n",
    "        Vt[2,:] *= -1\n",
    "        R = np.dot(Vt.T, U.T)\n",
    "\n",
    "    # compute the translation vector\n",
    "    t = mean_Y - R @ mean_X\n",
    "    \n",
    "    # return the transform c2Tc1\n",
    "    return SE3(t) * SE3(SO3(R))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn! - Implement the full 3D-3D motion estimation.\n",
    "\n",
    "Use the above method to estimate the trajectory for the complete Kitti sequence.\n",
    "\n",
    "Remember you have to first establish 3D points for each timestep. You have two options:\n",
    "\n",
    "Option 1:\n",
    " - get the full disparity map\n",
    " - calculate interest points in the left image only\n",
    " - look up the disparity for your interest points\n",
    " - calculate the 3D coordinates for your interestpoints using the disparity\n",
    "  \n",
    "Option2:\n",
    " - match interest points between left and right image and calculate the disparity only for the matches points\n",
    " - calculate 3D coordinates using the disparity\n",
    "  \n",
    "You then have to establish correspondences across two timesteps, e.g beore and after the camera moved. To do this, you want to match interest points between the left images before and after the motion.\n",
    "\n",
    "This gives you two sets of 3D points, including their correspondences.\n",
    "\n",
    "Arun's method above then gives you the transform c2Tc1. Remember that you want the inverse, c1Tc2 do describe the relative camera motion.\n",
    "Using the code from the 2D-2D method, comnbine the frame-to-frame motion estimates to calculate the motion for the complete trajectory.\n",
    "\n",
    "Remember that incorrect feature matches (either between left and right, or between two left images) will be very harmful for the motion estimation. Consider using the Fundamental Matrix estimation with a RANSAC loop to remove outlier matches.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motion Estimation from 3D-2D Correspondences\n",
    "\n",
    "3D-2D based motion estimation refers to finding the pose of a camera given a set of 3D points in the world and their corresponding 2D projections in the image. This problem is often referred to as Perspective-n-Point (PnP), and OpenCV provides a function called `cv2.solvePnP` to handle this.\n",
    "\n",
    "Using a stereo camera system for motion estimation offers some distinct advantages. With a stereo setup, you can obtain depth information for each image point by triangulating corresponding points from the two camera views. This allows you to estimate 3D points from 2D observations, which you can then use in the `solvePnP()` method to estimate the camera's motion.\n",
    "\n",
    "Here is an outline of the process:\n",
    "\n",
    "1. **Compute the Disparity Map:** For each pair of stereo images, compute the disparity map, which provides the displacement of each pixel between the two views.\n",
    "2. **Triangulate 3D Points:** Using the disparity map and the calibration data, you can triangulate the 3D coordinates of the points. Hint: Think about which coordinate system these points appear in and transform them into a different reference frame if needed.\n",
    "3. **Match 3D Points with 2D Points:** In the new image from one of the cameras (e.g. the left camera), detect and match 2D points with the previously triangulated 3D points.\n",
    "4. **Use `solvePnP()` to Estimate Motion:** With the matched 3D-2D points, use solvePnP() to estimate the motion of the camera.\n",
    "\n",
    "### Your Turn!\n",
    "Implement a frame-to-frame PnP-based motion estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enn583",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
