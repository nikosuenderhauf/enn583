{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1a2cf69-bf0c-488f-a6a3-230da9b76009",
   "metadata": {},
   "source": [
    "# ENN583: Week 11 Practical\n",
    "## Using GG-CNN to generate grasps!\n",
    "\n",
    "We will be working with the public implementation of GG-CNN which is available at this [link](https://github.com/dougsm/ggcnn).\n",
    "\n",
    "In particular, we're going to explore how you can:\n",
    "* load in the Project 2 data\n",
    "* adapt the data into the correct format for GG-CNN\n",
    "* test the data through GG-CNN to generate a rectangle grasp\n",
    "* evaluate whether the grasp is a 'True Positive' or not, to calculate Grasp detection rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e8c583-49b7-4b90-a280-59df1561e26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install shapely \n",
    "\n",
    "import torch\n",
    "import json\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.ndimage as ndimage\n",
    "\n",
    "import matplotlib.patches as patches\n",
    "import math\n",
    "\n",
    "from shapely.geometry import Polygon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc71b84-9242-4d63-acf9-952987447c4c",
   "metadata": {},
   "source": [
    "## Make sure you have the correct Project 2 data.\n",
    "\n",
    "Open the Project_2/download_data.ipynb and re-run the script to download the Project 2 data. This was changed on Thursday of Week 11 to use better depth data for grasp prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b489456-7e4b-4f74-85ff-edf49e4c0d57",
   "metadata": {},
   "source": [
    "### Loading in the Grasp prompts\n",
    "\n",
    "As is mentioned on the Project 2 instructions page on canvas, there is a 'validation_prompts.json' which contains a series of requested tests for you to perform. Each test contains a file name for an image in the dataset and a text prompt for the object that should be grasped. \n",
    "\n",
    "Below, let's load in the file and inspect some examples of the tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c7acde-abc5-43fe-a403-7484b3c004e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Project_2/validation_prompts.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(f'There are {len(data.keys())} tests')\n",
    "print('Here are the first 5 tests:')\n",
    "for i, test_name in enumerate(data.keys()):\n",
    "    print(f'{test_name}: {data[test_name]}')\n",
    "    if i >= 4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e72662c-915d-4dd3-8303-025bd032675b",
   "metadata": {},
   "source": [
    "### Test_1 - grasp the banana\n",
    "For most of this practical, we're going to focus on test_1. It will then be your job to connect this code with the object detection code, and automate testing over the many different files.\n",
    "\n",
    "As you can see above, test_1 is trying to grasp a banana in the image Graspnet_subset/validation_scenes/scene_0050/rgb/0000.png.\n",
    "\n",
    "Let's first load in and visualise the RGB and depth version of this image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305f398a-2a92-4fa8-9a0b-f494f292bc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_name = data['test_1'][0]\n",
    "rgb_name = f'../../enn583/Project_2/{im_name}'\n",
    "depth_name = rgb_name.replace('rgb', 'depth')\n",
    "\n",
    "\n",
    "im_rgb = cv2.imread(rgb_name)\n",
    "im_rgb = cv2.cvtColor(im_rgb, cv2.COLOR_BGR2RGB)\n",
    "im_depth = cv2.imread(depth_name,cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "\n",
    "plt.imshow(im_rgb)\n",
    "plt.title('RGB Image')\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(im_depth)\n",
    "plt.colorbar()\n",
    "plt.title('Depth Image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bf8ddd-7049-4241-bbdb-508a3bb09c28",
   "metadata": {},
   "source": [
    "### Adapting the data for GG-CNN\n",
    "\n",
    "GG-CNN wants to test on images of size 300 pixels by 300 pixels. It also works best when cropped around an object. \n",
    "\n",
    "To do this, we should:\n",
    "1. Crop the image to a patch that captures the object (i.e. banana). This crop should be AT MINIMUM 300x300.\n",
    "2. Then resize the crop to 300x300.\n",
    "\n",
    "Looking at the RGB image above, adapt the code below to crop the image around the object we want to grasp - the banana - and then the crop will be resized. You can use the code below to check how well you have cropped around the banana.\n",
    "\n",
    "**Consider: Here we are manually cropping around the banana... how could you leverage an object detector to do this automatically?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9649100b-2bbf-4b21-b5be-28b839822902",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adapt the code below to crop around the banana!\n",
    "x_offset = 0\n",
    "y_offset = 0\n",
    "crop_width = im_rgb.shape[1]\n",
    "crop_height = im_rgb.shape[0]\n",
    "\n",
    "crop_rgb = im_rgb[y_offset:y_offset+crop_height, x_offset:x_offset+crop_width]\n",
    "crop_depth = im_depth[y_offset:y_offset+crop_height, x_offset:x_offset+crop_width]\n",
    "\n",
    "#resize to always be 300x300\n",
    "crop_rgb = cv2.resize(crop_rgb, (300, 300))\n",
    "crop_depth = cv2.resize(crop_depth, (300, 300))\n",
    "\n",
    "plt.imshow(crop_rgb)\n",
    "plt.title('RGB Crop')\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(crop_depth)\n",
    "plt.colorbar()\n",
    "plt.title('Depth Crop')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e9f18b-c897-4d9f-abcb-8a9e4565d7bb",
   "metadata": {},
   "source": [
    "After a crop has been created and re-sized, GG-CNN also does some checks on the depth image to make sure it is in suitable format for the model. This code has nicely been formatted in a function for you. Read the code below to understand what is happening, and then try passing in the crop_depth and visualising the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f70fffb-eb30-4168-8f4e-193531a0b671",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_depth_image(depth_im, out_size = 300):\n",
    "    #add a border of a single pixel to the depth image\n",
    "    depth_crop = cv2.copyMakeBorder(depth_im, 1, 1, 1, 1, cv2.BORDER_DEFAULT)\n",
    "\n",
    "    #check for any nan values -- slightly dilate so not too noisy\n",
    "    depth_nan_mask = np.isnan(depth_crop).astype(np.uint8)\n",
    "    kernel = np.ones((3, 3),np.uint8)\n",
    "    depth_nan_mask = cv2.dilate(depth_nan_mask, kernel, iterations=1)\n",
    "\n",
    "    #any nan are set to have a value of 0 for now\n",
    "    depth_crop[depth_nan_mask==1] = 0\n",
    "\n",
    "    # Scale to keep as float, but has to be in bounds -1:1 to keep opencv happy.\n",
    "    depth_scale = np.abs(depth_crop).max()\n",
    "    depth_crop = depth_crop.astype(np.float32) / depth_scale  # Has to be float32, 64 not supported.\n",
    "\n",
    "    #all previous nan values are in-painted \n",
    "    depth_crop = cv2.inpaint(depth_crop, depth_nan_mask, 1, cv2.INPAINT_NS)\n",
    "\n",
    "    # Back to original size and value range (m from camera) -- essentially remove border and re-scale\n",
    "    depth_crop = depth_crop[1:-1, 1:-1]\n",
    "    depth_crop = depth_crop * depth_scale\n",
    "    \n",
    "    # Resize\n",
    "    depth_crop = cv2.resize(depth_crop, (out_size, out_size), cv2.INTER_AREA)\n",
    "\n",
    "    return depth_crop\n",
    "\n",
    "#Call the clean_depth_image function on the crop_depth and visualise the output -- has it changed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6c9103-2da9-42c8-83cb-079d471d7e58",
   "metadata": {},
   "source": [
    "We are almost ready to test the data! The last thing we need to do is format it for a Pytorch model by:\n",
    "1. Centering the values around 0 by subtracting the mean (i.e. pseudo-normalizing depth values)\n",
    "2. Converting the depth crop to a Tensor and moving it to the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bb9684-57ac-49f4-9404-aeaf5617153f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for torch models -- center the depth around 0\n",
    "depth_norm = np.clip((crop_depth - crop_depth.mean()), -1, 1)\n",
    "\n",
    "#convert to tensor and move to GPU\n",
    "#reshaping to be batch x channels x width x height\n",
    "depth_tensor = torch.from_numpy(depth_norm.reshape(1, 1, 300, 300).astype(np.float32)).cuda()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623ac305-4fd7-4515-a69a-6382e4aa5253",
   "metadata": {},
   "source": [
    "### Test the data through GG-CNN to generate a rectangle grasp!\n",
    "\n",
    "Below, we are loading in the GG-CNN model and passing our depth image through to get an output! \n",
    "\n",
    "Have a close look at the output -- there are a number of segmentation maps for the quality of each pixel for grasping, the angle for grasping, and the width of the grasp.\n",
    "\n",
    "Adapt the cell below to visualise these segmentation maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b32c91-19ab-4a01-b5d8-de25da07ae65",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = torch.load('ggcnn2_weights_cornell/epoch_50_cornell')\n",
    "\n",
    "with torch.no_grad():\n",
    "    quality, cos, sin, width = model(depth_tensor)\n",
    "\n",
    "quality = quality.cpu().numpy()\n",
    "cos = cos.cpu().numpy()\n",
    "sin = sin.cpu().numpy()\n",
    "width = width.cpu().numpy()\n",
    "\n",
    "ang_radians = np.arctan2(sin, cos) / 2.0\n",
    "\n",
    "#Use plt.imshow() to visualise the results of the segmentation outputs. You may want to do this alongside showing the crop_rgb to help interpret."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add40de4-6557-4c27-95a0-96a36021a109",
   "metadata": {},
   "source": [
    "You might notice a couple of things that don't make a lot of sense in the above:\n",
    "1. There are some weird noisy patches and artifacts -- it's not very smooth!\n",
    "2. We want to know the width of the gripper in pixels, but the values do not go very high.\n",
    "\n",
    "This is because there is actually some post-processing that needs to happen! Read the function below to see the post-processing that should be done, and then re-visualise each of the grasp maps.\n",
    "\n",
    "Each of the postprocessing steps are recommended by the authors of the GG-CNN paper -- ask yourself, do they make sense? Why would they be beneficial?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d716dcbb-50c9-4c73-b19b-04de296806bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_segmentations(quality_seg, angle_seg, width_seg):\n",
    "    filters = (2, 2, 1)\n",
    "\n",
    "    #width is scaled by 150 pixels\n",
    "    width_seg = width_seg * 150.0\n",
    "\n",
    "    #apply a smoothing filter to each of the segmentation masks\n",
    "    if filters[0]:\n",
    "        quality_seg = ndimage.filters.gaussian_filter(quality_seg, filters[0]) \n",
    "    if filters[1]:\n",
    "        angle_seg = ndimage.filters.gaussian_filter(angle_seg, filters[1])\n",
    "    if filters[2]:\n",
    "        width_seg = ndimage.filters.gaussian_filter(width_seg, filters[2])\n",
    "\n",
    "    #clip the quality segmentation mask to between 1 and 0.999\n",
    "    quality_seg = np.clip(quality_seg, 0, 1.0-1e-3)\n",
    "\n",
    "    return quality_seg, angle_seg, width_seg\n",
    "\n",
    "\n",
    "#pass in the outputs to the above function and then visualise the postprocessed results.\n",
    "quality_final, ang_radians_final, width_final = ....\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0dae634-4286-456f-a5d5-867410bea55e",
   "metadata": {},
   "source": [
    "Now, we can extract a grasp in terms of a center x, center y, gripper width, and angle from our segmentation masks. \n",
    "\n",
    "Below, let's first check where the most confident grasp predictions are based on the quality segmentation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a29cf18-2e50-4011-9f6f-e26e0f39d749",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rotated_rectangle_corners(center_x, center_y, width, angle_rad):\n",
    "    # Given a grasp prediction in the format required by the assignment, this function will convert it into \n",
    "    # 4 rotated rectangle corners that can be drawn onto an image and visualised. Or compared with a GT grasp\n",
    "    # for performance evaluation.\n",
    "    \n",
    "    #height is fixed based on the gripper \n",
    "    height = 30\n",
    "    \n",
    "    # Extract center coordinates\n",
    "    cx, cy = center_x, center_y\n",
    "\n",
    "    # Calculate half dimensions\n",
    "    half_width = width / 2\n",
    "    half_height = height / 2\n",
    "    \n",
    "    # Calculate the original corner positions relative to the center\n",
    "    corners = [\n",
    "        (-half_width, -half_height),  # Bottom-left\n",
    "        (half_width, -half_height),   # Bottom-right\n",
    "        (half_width, half_height),    # Top-right\n",
    "        (-half_width, half_height)    # Top-left\n",
    "    ]\n",
    "    \n",
    "    # Apply rotation and translation to each corner\n",
    "    rotated_corners = []\n",
    "    for x, y in corners:\n",
    "        # Calculate rotated positions\n",
    "        rotated_x = cx + (x * math.cos(angle_rad)) - (y * math.sin(angle_rad))\n",
    "        rotated_y = cy + (x * math.sin(angle_rad)) + (y * math.cos(angle_rad))\n",
    "        rotated_corners.append((rotated_x, rotated_y))\n",
    "    \n",
    "    return rotated_corners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cd1ac1-b650-4dba-84b0-60ed0aadfd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_final = quality_final.squeeze()\n",
    "width_final = width_final.squeeze()\n",
    "ang_radians_final = ang_radians_final.squeeze()\n",
    "\n",
    "most_confident_pixels = np.dstack(np.unravel_index(np.argsort(quality_final.ravel())[::-1], quality_final.shape))[0]\n",
    "#print the 10 most confident pixels\n",
    "print(most_confident_pixels[:10])\n",
    "\n",
    "plt.imshow(quality_final)\n",
    "plt.colorbar()\n",
    "plt.title('Postprocessed Quality')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37300eba-eb21-4194-85f2-2806c0fce83a",
   "metadata": {},
   "source": [
    "The code below will take **the most confident** grasp as the prediction, drawing the rotated rectangle.\n",
    "\n",
    "**Consider: You want to grab the grasp for a specific object -- how could you do this using your results from your object detector?**\n",
    "\n",
    "Adapt the code below to try and only take a grasp that is on a banana. You can hand-code this at first but make sure to think how you would do it automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d72f8c-a6a5-4f26-abac-3d3fc351417f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_pixel = most_confident_order[0]\n",
    "u = max_pixel[1]\n",
    "v = max_pixel[0]\n",
    "ang = ang_radians_final[max_pixel[0], max_pixel[1]]\n",
    "width = width_final[max_pixel[0], max_pixel[1]]\n",
    "\n",
    "#convert to grasp rectangle\n",
    "grasp_rect = get_rotated_rectangle_corners(u, v, width, ang)\n",
    "\n",
    "#Plot the image with the grasp\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(crop_rgb)\n",
    "polygon = patches.Polygon(grasp_rect, closed=True, edgecolor='red', linewidth=2, fill=None)\n",
    "ax.add_patch(polygon)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e887d22d-6d91-4bcc-9e5a-5911754b9ecb",
   "metadata": {},
   "source": [
    "Now we have the grasp in terms of our image crop -- but for evaluation, we need to convert it back into the global image coordinates! Use the code below to do that.\n",
    "\n",
    "**Note: this relies on knowing the crop_width, crop_height and x,y offsets from when we cropped and resized.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65e866f-5860-44d4-95fc-8c8e578ba717",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_w = crop_width/300\n",
    "delta_h = crop_height/300\n",
    "\n",
    "# Convert max_pixel back to uncropped/resized image coordinates.\n",
    "u = u*delta_w + x_offset\n",
    "v = v*delta_h + y_offset\n",
    "width = width*delta_w\n",
    "\n",
    "#convert to grasp rectangle\n",
    "grasp_rect = get_rotated_rectangle_corners(u, v, width, ang)\n",
    "\n",
    "#Plot the image with the grasp\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(im_rgb)\n",
    "polygon = patches.Polygon(grasp_rect, closed=True, edgecolor='red', linewidth=2, fill=None)\n",
    "ax.add_patch(polygon)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c10b5d5-329c-4bce-ba76-a494f557cc66",
   "metadata": {},
   "source": [
    "## Evaluate whether the grasp is a 'True Positive' or not\n",
    "\n",
    "Let's load in all the GT grasp labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce0707c-5eb0-432e-a30d-83791a003c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in gt grasp labels\n",
    "with open('../../enn583/Project_2/validation_grasp_labels.json', 'r') as f:\n",
    "    gt_grasps = json.load(f)\n",
    "\n",
    "test_1_grasp = gt_grasps['test_1']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea48b7f-b511-47df-8625-b035cef180ca",
   "metadata": {},
   "source": [
    "For a grasp to be a True Positive, it needs to both:\n",
    "1. Have an IoU > 0.25 with a GT grasp rectangle, AND\n",
    "2. Have a grasp angle within 20deg (0.349 radians) of the GT grasp angle.\n",
    "\n",
    "The code below is testing this condition for the first grasp label in test_1_grasp.\n",
    "\n",
    "Adapt the code to check over all grasps in test_1_grasp and see if the grasp is a True Positive or False Positive. **It only needs to meet the conditions with a single GT grasp to be a TP.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1c561c-1316-4163-8d2d-d41e17d703b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_grasp = test_1_grasp[0]\n",
    "gt_angle = -np.arctan((g[0][0] - g[3][0]) / (g[0][1] - g[3][1])) #use this to calculate angle of rotation of rectangle\n",
    "\n",
    "#Calculate IoU\n",
    "polygon1 = Polygon(grasp_rect)\n",
    "polygon2 = Polygon(gt_grasp)\n",
    "intersect = polygon1.intersection(polygon2).area\n",
    "union = polygon1.union(polygon2).area\n",
    "iou = intersect / union\n",
    "\n",
    "if iou > 0.25:\n",
    "    if np.abs(gt_angle - angle) < 0.349:\n",
    "        print('TP')\n",
    "    else:\n",
    "        print('FP')\n",
    "else:\n",
    "    print('FP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5addd91-b94f-45cd-a69a-9ff09bdb2ce7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
