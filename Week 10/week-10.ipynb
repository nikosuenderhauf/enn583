{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6383be93-a913-4451-afa8-5539530fadb1",
   "metadata": {},
   "source": [
    "# ENN583 Week 10 - Object Detection\n",
    "\n",
    "This week's practical will explore how you can use 3 different object detectors from PyTorch to test an image, comparing how these detectors perform in different challenging conditions.\n",
    "\n",
    "In particular, we'll compare:\n",
    "* [Faster R-CNN](https://arxiv.org/pdf/1506.01497.pdf) -- one of the classic object detectors, a two-stage anchor-box architecture\n",
    "* [RetinaNet](https://arxiv.org/pdf/1708.02002.pdf) -- a high performing one-stage anchor-box architecture\n",
    "* [FCOS](https://arxiv.org/pdf/1904.01355.pdf) -- a popular one-stage anchor-free architecture \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0d3bb2-13a0-4e4b-87bf-bbceeda05e10",
   "metadata": {},
   "source": [
    "## Step 1: Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775dd41b-fa2d-4256-8482-2311729ac53a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.io.image import read_image\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights, fcos_resnet50_fpn, FCOS_ResNet50_FPN_Weights, retinanet_resnet50_fpn_v2, RetinaNet_ResNet50_FPN_V2_Weights\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from colormap import sample_colors\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b558943d-d2ce-4c9c-8bf2-7f0e33f7da12",
   "metadata": {},
   "source": [
    "## Step 2: Load the detectors from PyTorch\n",
    "\n",
    "PyTorch has a number of pre-trained models available, including object detectors trained on COCO! \n",
    "\n",
    "You can see these [here](https://pytorch.org/vision/main/models.html#object-detection-instance-segmentation-and-person-keypoint-detection).\n",
    "\n",
    "As mentioned, we will compare Faster R-CNN (v2), RetinaNet (v2), and FCOS. Compare the 'Box MAP' of each of these models as listed on the linked Pytorch page -- which model do you expect to perform the best? Also consider the model number of parameters, and GFLOPS -- is there a big difference?\n",
    "\n",
    "Below, we have built off the pytorch example code to visualise how to use an object detector. Note the key steps:\n",
    "1. Initialise the model, load the weights and set into 'eval' mode\n",
    "    * Note how we need to set a box_score_thresh -- this is the minimum confidence threshold for a detection to be valid. Let's start with this low, and we can always filter detections later\n",
    "2. Load the data transforms necessary -- these typically ensure the image is the correct size and is normalized\n",
    "3. Preprocess the data using the data transform -- pretty self-explanatory\n",
    "4. Test the data with the model\n",
    "\n",
    "We're printing the output from the object detector -- how is it describing detections as an output? Can you recognise the different elements of the detection? How is the bounding box parameterised?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6695ccf8-2f13-48bd-895a-5cec1768c947",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img = read_image(\"images/coco/000000017029.jpg\")\n",
    "\n",
    "# Step 1: Initialize model with the best available weights\n",
    "weights_frcnn = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\n",
    "frcnn = fasterrcnn_resnet50_fpn_v2(weights=weights_frcnn, box_score_thresh=0.2)\n",
    "frcnn.eval()\n",
    "\n",
    "# Step 2: Initialize the inference transforms\n",
    "preprocess = weights_frcnn.transforms()\n",
    "\n",
    "# Step 3: Apply inference preprocessing transforms\n",
    "batch = [preprocess(img)]\n",
    "\n",
    "# Step 4: Use the model and visualize the prediction\n",
    "prediction = frcnn(batch)[0]\n",
    "\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0578a842-7789-4b6c-81bf-196b5d3ab5ee",
   "metadata": {},
   "source": [
    "## Step 3: Visualise the predictions\n",
    "\n",
    "Below, I've created a **draw_detections** function that you can use to visualise results from the Pytorch detectors. You need to provide the image, the model prediction, and the name of the detector. Read through the function to understand how it works, and then visualise the results from the last test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62faef42-bf53-4fb0-8f38-81cea5879b93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "coco_categories = weights_frcnn.meta[\"categories\"]\n",
    "\n",
    "def draw_detections(img, prediction, detectorName = 'Faster R-CNN'):\n",
    "\n",
    "    #extract the labels and scores -- combine these into text that can be printed on the image\n",
    "    labels = [coco_categories[i] for i in prediction[\"labels\"]]\n",
    "    scores = [f'{int(100.*torch.round(s, decimals=2))}%' for s in prediction['scores']]\n",
    "    print_txt = [f'{labels[i]}: {scores[i]}' for i in range(len(prediction['scores']))]\n",
    "\n",
    "    # picks colors that are visually distinct to draw on the image\n",
    "    color_list = sample_colors(len(scores), rgb = True)\n",
    "    color_list_int = [(int(c[0]), int(c[1]), int(c[2])) for c in color_list]\n",
    "\n",
    "    #creates a tensor image with the bboxes drawn on the image\n",
    "    box = draw_bounding_boxes(img, boxes=prediction[\"boxes\"],\n",
    "                              labels=print_txt,\n",
    "                              colors=color_list_int,\n",
    "                              width=6, font_size=50)\n",
    "    \n",
    "    #put the image in format compatible with matplotlib\n",
    "    box = box.numpy()\n",
    "    box = np.swapaxes(box, 0, 1)\n",
    "    box = np.swapaxes(box, 1, 2)\n",
    "    \n",
    "    #draw with matplotlib\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(box)\n",
    "    ax.set_title(detectorName)\n",
    "    \n",
    "    #text is hard to read -- create a custom legend\n",
    "    custom_lines = []\n",
    "    for c in color_list_int:\n",
    "        c_plt = (c[0]/255, c[1]/255, c[2]/255)\n",
    "        custom_lines += [Line2D([0], [0], color = c_plt, lw = 4)]\n",
    "    fig.legend(custom_lines, print_txt, loc = 'outside right center')\n",
    "                            \n",
    "    plt.show()\n",
    "    \n",
    "draw_detections(img, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2040736-f838-4705-9b1d-b518e714e82e",
   "metadata": {},
   "source": [
    "## Step 4: Compare to RetinaNet and FCOS\n",
    "\n",
    "In the cell below, I've loaded the 3 models we want to compare. I've also created a loop that tests each of these models and then displays the results. \n",
    "\n",
    "Looking at the results -- what differences can you see between how the models behave for this image?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd77f422-d316-4af8-92de-5a69f0fb2315",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1: Initialize model with the best available weights\n",
    "weights_frcnn = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\n",
    "frcnn = fasterrcnn_resnet50_fpn_v2(weights=weights_frcnn, box_score_thresh=0.2)\n",
    "frcnn.eval()\n",
    "\n",
    "weights_fcos = FCOS_ResNet50_FPN_Weights.DEFAULT\n",
    "fcos = fcos_resnet50_fpn(weights=weights_fcos, box_score_thresh=0.2)\n",
    "fcos.eval()\n",
    "\n",
    "weights_retinanet = RetinaNet_ResNet50_FPN_V2_Weights.DEFAULT\n",
    "retinanet = retinanet_resnet50_fpn_v2(weights=weights_retinanet, box_score_thresh=0.2)\n",
    "retinanet.eval()\n",
    "\n",
    "models = {'F-RCNN': frcnn, 'RetinaNet': retinanet, 'FCOS': fcos}\n",
    "weights = {'F-RCNN': weights_frcnn, 'RetinaNet': weights_retinanet, 'FCOS': weights_fcos}\n",
    "\n",
    "\n",
    "img = read_image(\"images/coco/000000017029.jpg\")\n",
    "\n",
    "for modelName in models.keys():\n",
    "    model = models[modelName]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        preprocess = weights[modelName].transforms()\n",
    "\n",
    "        batch = [preprocess(img)]\n",
    "\n",
    "        prediction = model(batch)[0]\n",
    "\n",
    "    draw_detections(img, prediction, modelName)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b06b1ca-2a1e-4cf8-804a-ed66bda3c322",
   "metadata": {},
   "source": [
    "A couple of trends should be evident from the above:\n",
    "1. We set a minimum confidence threshold for all detectors of 0.2 -- but RetinaNet is producing a huge number of detections with a confidence less than that? What's going on? As far as I can tell, this is a bug in the Pytorch code.\n",
    "2. FCOS produces **way** more detections than Faster R-CNN with a confidence threshold above 0.2. \n",
    "3. Even though Faster R-CNN produces less detections than RetinaNet and FCOS, it is still producing some low confidence detections.\n",
    "\n",
    "## Step 5: Adding reasonable confidence thresholds\n",
    "\n",
    "**Your turn:** Looking at the results above, we need to pick a reasonable minimum confidence threshold that seems to keep the correct detections, and throw away the incorrect detections. This seems to be different for every detector. For each detector, choose a minimum reasonable confidence threshold, then go to the **draw_detections** function, and add an optional argument that lets you filter and only draw detections above this threshold. Run the cell above again until you seem to get as reasonable as possible results for all detectors.\n",
    "\n",
    "## Step 6: Checking performance for multiple images\n",
    "\n",
    "**Your turn:** Adapt the above code to test all images in the 'images/coco' folder. You can use this **glob.glob** function to achieve this.\n",
    "\n",
    "Once again, compare the detectors. You may need to adjust the minimum reasonable confidence threshold for each detector. Consider:\n",
    "* How do the confidence thresholds for detections differ between each detector?\n",
    "* How is the localisation of each detection?\n",
    "* How is the recall of each detector (e.g. it detects all the relevant objects)?\n",
    "* How is the precision of each detector (e.g. it doesn't incorrectly detect any objects)?\n",
    "\n",
    "## Step 7: Object Detection in the Wild!\n",
    "\n",
    "**Your turn:** Pick an object category (e.g. dog is an easy one), and search the internet to find ***challenging*** examples of that object, e.g. weird viewpoints, unusual-looking examples, poor illumination, occlusions, cluttered scenes, small objects. Place these in the 'challenging' image folder, then test the detectors on these images and compare their performance. \n",
    "\n",
    "What happens? Does the detector still detect the object?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
