{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENN583 - Week 2 - Practical\n",
    "\n",
    "In this week's Prac we will be explore OpenCV's functions for interest points detection, description, and matching.\n",
    "\n",
    "\n",
    "We will be working with one of the Kitti dataset sequences, the same we used last week. If you have not already downloaded it, go back to last week's Prac.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the KITTI Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if pykitti is installed, if not use pip to set it up\n",
    "try:\n",
    "    import pykitti\n",
    "except:\n",
    "    !pip install pykitti\n",
    "    import pykitti\n",
    "\n",
    "# Read the dataset sequence we just downloaded\n",
    "basedir = '../kitti'\n",
    "date = '2011_09_26'\n",
    "drive = '0035'\n",
    "\n",
    "# The 'frames' argument is optional - default: None, which loads the whole dataset.\n",
    "# data = pykitti.raw(basedir, date, drive, frames=range(0, 50, 5))\n",
    "data = pykitti.raw(basedir, date, drive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open and Display Images from the Stereo Camera of the KITTI Sequence\n",
    "\n",
    "Here is a reminder how we can get access to the left and right images from the colour stereo camera through the dataset class.\n",
    "\n",
    "See last week's Prac notebook for a reminder of the different types of sensor data and ground truth pose that is stored in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# get the left and right stereo images from the 10th frame\n",
    "left, right = data.get_rgb(10)\n",
    "\n",
    "# use plt to show them side by side\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(left)\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(right)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract FAST Features\n",
    "\n",
    "As a first demo, let us detect FAST corners in both images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the left and right image of the 10th stereo pair in this sequence and convert them to greyscale.\n",
    "left, right = data.get_rgb(10)\n",
    "left = cv2.cvtColor(np.array(left), cv2.COLOR_RGB2GRAY)  \n",
    "right = cv2.cvtColor(np.array(right), cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "# detect FAST corners in both images\n",
    "fast = cv2.FastFeatureDetector_create(threshold=25, nonmaxSuppression=True)\n",
    "kp1 = fast.detect(left, None)\n",
    "kp2 = fast.detect(right, None)\n",
    "\n",
    "# kp1 and kp2 are lists of keypoints, each of the class cv2.KeyPoint\n",
    "# see the documentation at https://docs.opencv.org/3.4/d2/d29/classcv_1_1KeyPoint.html for all the class members\n",
    "\n",
    "# print the number of keypoints detected in each image\n",
    "print(f'Left: {len(kp1)} keypoints')\n",
    "print(f'Right: {len(kp2)} keypoints')\n",
    "\n",
    "# let us inspect the interesting members of a keypoint (i.e. the ones that are not starting with an underscore)\n",
    "print(f'Interesting Keypoint class members: {[m for m in dir(kp1[0]) if not m.startswith(\"_\")]}')\n",
    "\n",
    "# to get access to all of the keypoint coordinates, we can access their individual .pt members like this:\n",
    "pt1 = np.array([x.pt for x in kp1])\n",
    "pt2 = np.array([x.pt for x in kp2])\n",
    "\n",
    "# we can manually draw the keypoints into the images\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(pt1[:,0], pt1[:,1], 'r.', markersize=1)\n",
    "plt.title('Left Image')\n",
    "plt.imshow(left, cmap='gray')\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "plt.imshow(right, cmap='gray')\n",
    "plt.plot(pt2[:,0], pt2[:,1], 'r.', markersize=1)\n",
    "plt.title('Right Image')\n",
    "plt.suptitle('Keypoints detected with FAST')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# for a quicker way to display keypoints, we can use the drawKeypoints function of OpenCV\n",
    "left_corners = cv2.drawKeypoints(left, kp1, None, color=(255,0,0))\n",
    "right_corners = cv2.drawKeypoints(right, kp2, None, color=(255,0,0))\n",
    "\n",
    "# display the images\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(211)\n",
    "plt.imshow(left_corners)\n",
    "plt.title('Left Image')\n",
    "plt.subplot(212)\n",
    "plt.imshow(right_corners)\n",
    "plt.title('Right Image')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Different Interest Point Detectors\n",
    "\n",
    "Let us now work with different detectors. \n",
    "\n",
    "Modify the code below and extract the following features:\n",
    " - SIFT (see https://docs.opencv.org/4.7.0/da/df5/tutorial_py_sift_intro.html and https://docs.opencv.org/4.7.0/d7/d60/classcv_1_1SIFT.html)\n",
    " - ORB (see https://docs.opencv.org/4.7.0/d1/d89/tutorial_py_orb.html and https://docs.opencv.org/4.7.0/db/d95/classcv_1_1ORB.html)\n",
    "\n",
    "Display them and compare. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the left and right image of the 10th stereo pair in this sequence and convert them to greyscale.\n",
    "left, right = data.get_rgb(10)\n",
    "left = cv2.cvtColor(np.array(left), cv2.COLOR_RGB2GRAY)  \n",
    "right = cv2.cvtColor(np.array(right), cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "# detect SIFT features in both images\n",
    "sift = cv2.xfeatures2d.SIFT_create()       # Your turn! Explore different parameters you can pass into the SIFT constructor\n",
    "kp1 = sift.detect(left, None)\n",
    "kp2 = sift.detect(right, None)\n",
    "\n",
    "# visualise the keypoints, including their scale and orientation\n",
    "img = cv2.drawKeypoints(left, kp1, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(2,1,1)\n",
    "plt.title('Left Image')\n",
    "plt.imshow(img, cmap='gray')\n",
    "\n",
    "img = cv2.drawKeypoints(right, kp2, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "plt.subplot(2,1,2)\n",
    "plt.imshow(img)\n",
    "plt.title('Right Image')\n",
    "plt.suptitle('Keypoints detected with SIFT')\n",
    "plt.show()\n",
    "\n",
    "# ================================ #\n",
    "# Your turn! Change the feature detector to ORB and see what happens\n",
    "\n",
    "orb = cv2.ORB_create()               # Your turn! Explore different parameters you can pass into the SIFT constructor\n",
    "\n",
    "# YOUR TURN! CONTINUE YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Feature Matching\n",
    "\n",
    "Let's calculate some descriptors and try matching features between different images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the left and right image of the 10th stereo pair in this sequence and convert them to greyscale.\n",
    "left, right = data.get_rgb(10)\n",
    "left = cv2.cvtColor(np.array(left), cv2.COLOR_RGB2GRAY)  \n",
    "right = cv2.cvtColor(np.array(right), cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "# detect SIFT features in both images\n",
    "# this time we also calculate their descriptors\n",
    "sift = cv2.xfeatures2d.SIFT_create()  \n",
    "kp1, desc1 = sift.detectAndCompute(left, None)     \n",
    "kp2, desc2 = sift.detectAndCompute(right, None)\n",
    "\n",
    "# the simplest matching method is the Brute Force Matcher, cv2.BFMatcher (https://docs.opencv.org/4.7.0/d3/da1/classcv_1_1BFMatcher.html)\n",
    "# create the BFMatcher object. Notice how we use the crossCheck flag to enable the \n",
    "bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n",
    "\n",
    "# match the descriptors from both images\n",
    "matches = bf.match(desc1, desc2)\n",
    "\n",
    "# matches is of type DMatch\n",
    "print(f'There are {len(matches)} matches. They are of type {type(matches[0])}')\n",
    "print(f'Each match has the following attributes: {[a for a in dir(matches[0]) if not a.startswith(\"_\")]}')\n",
    "\n",
    "# there is a convenient function that lets us display the matches\n",
    "# we can specify the number of matches we want to display\n",
    "# here we display the strongest 50 matches\n",
    "# notice that the matches are sorted by distance\n",
    "matches = sorted(matches, key = lambda x:x.distance)    # quickly sort the matches based on distance, smallest distance first\n",
    "img = cv2.drawMatches(left,kp1,right,kp2,matches[:50], None, flags=cv2.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS)\n",
    "\n",
    "plt.figure(figsize=(40,20))\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# YOUR TURN! Plot a histogram of the distances of the matches. What do you notice?\n",
    "\n",
    "\n",
    "# ... your code here ...\n",
    "\n",
    "\n",
    "# Your turn!\n",
    "# Now calculate ORB features and descriptors again, and match them using the Brute Force Matcher\n",
    "# Pay attention to the distance metric we use for the matcher. Is the L2 Norm still the best choice?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually Extract BRIEF Descriptors\n",
    "\n",
    "We can also extract BRIEF descriptors around any location in an image. Let's try this!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the left and right image of the 10th stereo pair in this sequence and convert them to greyscale.\n",
    "left, right = data.get_rgb(10)\n",
    "left = cv2.cvtColor(np.array(left), cv2.COLOR_RGB2GRAY)  \n",
    "right = cv2.cvtColor(np.array(right), cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "# detect FAST corners in both images\n",
    "fast = cv2.FastFeatureDetector_create(threshold=25, nonmaxSuppression=True)\n",
    "kp1 = fast.detect(left, None)\n",
    "kp2 = fast.detect(right, None)\n",
    "\n",
    "# Initiate BRIEF extractor\n",
    "brief = cv2.xfeatures2d.BriefDescriptorExtractor_create()\n",
    "\n",
    "# compute the descriptors with BRIEF\n",
    "kp1, desc1 = brief.compute(left, kp1)\n",
    "kp2, desc2 = brief.compute(right, kp2)\n",
    "\n",
    "# match the descriptors from both images\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "matches = bf.match(desc1, desc2)\n",
    "\n",
    "# draw the first 100 matches\n",
    "matches = sorted(matches, key = lambda x:x.distance)    # quickly sort the matches based on distance, smallest distance first\n",
    "img = cv2.drawMatches(left,kp1,right,kp2,matches[:100], None, flags=cv2.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS)\n",
    "plt.figure(figsize=(40,20))\n",
    "plt.imshow(img)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Matching Features between Consecutive Frames in Time\n",
    "\n",
    "So far, we have matched keypoints between the left and right images of a stereo camera.\n",
    "Let's try doing this between the left frames of the camera at different positions along the trajectory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the left images a few frames apart\n",
    "left1, _ = data.get_rgb(10)\n",
    "left2, _ = data.get_rgb(15)\n",
    "\n",
    "left1 = cv2.cvtColor(np.array(left1), cv2.COLOR_RGB2GRAY)  \n",
    "left2 = cv2.cvtColor(np.array(left2), cv2.COLOR_RGB2GRAY)  \n",
    "\n",
    "# use plt to show them side by side\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(left1, cmap='gray')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(left2, cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "# detect and match ORB features\n",
    "orb = cv2.ORB_create()               # Your turn! Explore different parameters you can pass into the SIFT constructor\n",
    "kp1, desc1 = orb.detectAndCompute(left1, None)\n",
    "kp2, desc2 = orb.detectAndCompute(left2, None)\n",
    "\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "matches = bf.match(desc1, desc2)\n",
    "matches = sorted(matches, key = lambda x:x.distance)    # quickly sort the matches based on distance, smallest distance first\n",
    "\n",
    "img = cv2.drawMatches(left1,kp1,left2,kp2,matches[:100], None, flags=cv2.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS)\n",
    "\n",
    "plt.figure(figsize=(40,20))\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# YOUR TURN!\n",
    "# Try matching from frame 10 to frames increasingly further away (15, 20, 25, 50, ...)\n",
    "# What do you notice? Is there a point where the matches start to get worse? Why do you think that is?\n",
    "# How do the distances of the found matches behave? Do they increase? Decrease? Stay the same?\n",
    "# Repeat this experiment with ORB and SIFT. Which one is more robust to changes in viewpoint?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enn583",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
