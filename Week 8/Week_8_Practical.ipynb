{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a0808b7-1860-417e-b37f-e0253084b2e7",
   "metadata": {},
   "source": [
    "# ENN583 Week 8: When Computer Vision Fails For Robotics\n",
    "\n",
    "In the lecture, we discussed how models developed for a dataset (computer vision models) can fall short when tested in an environment onboard a robot (robotic vision application).\n",
    "\n",
    "In particular, we talked about:\n",
    "* measuring performance with different metrics -- accuracy, precision, recall\n",
    "* confidence calibration\n",
    "* feature shift\n",
    "* class shift\n",
    "\n",
    "In this notebook, you'll explore each of these ideas for an image classification task. We're going to utilise a ResNet50 classification model, **pretrained on computer vision dataset ImageNet**, evaluating its performance and testing its limits.\n",
    "\n",
    "Read through the notebook below and complete any missing sections. If you're attending the practical -- ask questions if you get stuck, and I will pick some points to guide you through the answers. If you could not attend the practical, you can always email d24.miller@qut.edu.au if you got stuck in any sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aef2232-4bf0-4f9d-90f0-c6dde928448a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#import torch which has many of the functions to build deep learning models and to train them\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "#import torchvision, which was lots of functions for loading and working with image data\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "#this is a nice progress bar representation that will be good to measure progress during testing\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e905fb38-7513-4103-bc15-a0949d6517b7",
   "metadata": {},
   "source": [
    "## 1. ImageNet Validation Data\n",
    "\n",
    "We're going to be exploring a model pretrained on ImageNet -- ImageNet has 1000 different classes and internet-sourced images. We will test on a _subset_ of those classes in this practical, specifically:\n",
    "* backpack\n",
    "* ballpoint pen\n",
    "* cellphone\n",
    "* computer mouse\n",
    "* laptop computer\n",
    "* wallet\n",
    "* water bottle\n",
    "\n",
    "You can view the entire set of classes in ImageNet by opening the _imagenet\\_labels.py_ file.\n",
    "\n",
    "## Loading the Data\n",
    "\n",
    "This step has 2 key parts:\n",
    "1. Create default transformations to apply to the data. The below 3 steps are very standard, and should always be used.\n",
    "    There are a number of transformations we will consider here, these include:\n",
    "    1. [transforms.ToTensor()](https://pytorch.org/vision/stable/generated/torchvision.transforms.ToTensor.html) -- this converts a PIL image or numpy array to a tensor while scaling the pixel values to the range [0, 1].\n",
    "    2. [transforms.Resize()](https://pytorch.org/vision/stable/generated/torchvision.transforms.Resize.html) -- this resizes an input image to the specified size (height, width).\n",
    "    Resize is important as it ensures the dimensions remain compatible throughout the network, allowing proper operations at each layer and maintaining the required dimensions for the final fully connected layers in the network.\n",
    "    3. [transforms.Normalize()](https://pytorch.org/vision/stable/generated/torchvision.transforms.Normalize.html) -- this standardizes the pixel values of a tensor image by subtracting the mean and dividing by the standard deviation along the input channels.\n",
    "    \n",
    "    You can then use [transforms.Compose](https://pytorch.org/vision/main/generated/torchvision.transforms.Compose.html#torchvision.transforms.Compose) to sequentially chain multiple transforms together.\n",
    "\n",
    "2. Load the datasets in with [torchvision.datasets.ImageFolder](https://pytorch.org/vision/stable/generated/torchvision.datasets.ImageFolder.html) -- this loads image datasets from folders, assigning labels automatically based on subdirectories, making it convenient for tasks like image classification.\n",
    "\n",
    "**Why Resize to 224x224?**\n",
    "Many popular pre-trained models, such as AlexNet, VGG, and ResNet, were trained on the ImageNet dataset, which used images of size 224x224 pixels. We will use a ResNet architecture pre-trained on ImageNet, so will use this value.\n",
    "\n",
    "**How do we pick the normalization values?**\n",
    "We can use the actual underlying statistics in our training data, or we can use the values from the ImageNet dataset (millions of images).\n",
    "\n",
    "Below, we're using ImageFolder to load in our imagenet_subset dataset with a transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2b78ee-a291-442f-a265-fb9fb47d4e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_means = (0.485, 0.456, 0.406)\n",
    "imagenet_stds = (0.229, 0.224, 0.225)\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Resize((224, 224), antialias = True), \n",
    "     transforms.Normalize(imagenet_means, imagenet_stds)])\n",
    "\n",
    "test_data = torchvision.datasets.ImageFolder('imagenet_subset', transform = transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6972b9a6-8e5c-4144-b29b-bac67a4ad8cf",
   "metadata": {},
   "source": [
    "## Visualising the data\n",
    "\n",
    "It's usually also a good idea to look at some of the data that we're testing. Let's do this below with matplotlib.pyplot for visualisation, showing an image for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9e6998-0ec9-43c2-9fd1-6ecfec597464",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img, lbl = None):\n",
    "    img = img  * torch.Tensor(imagenet_stds).unsqueeze(1).unsqueeze(2) + torch.Tensor(imagenet_means).unsqueeze(1).unsqueeze(2)    # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    if lbl != None:\n",
    "        plt.title(lbl)\n",
    "    plt.show()\n",
    "\n",
    "class_lbl_to_idx = test_data.class_to_idx\n",
    "class_idx_to_lbl = {v: k for k, v in class_lbl_to_idx.items()}\n",
    "\n",
    "for cls in range(7):\n",
    "    for im, cls_idx in test_data:\n",
    "        if cls_idx != cls:\n",
    "            continue\n",
    "            \n",
    "        cls_lbl = class_idx_to_lbl[cls_idx]\n",
    "        imshow(im, cls_lbl)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e026448-fd63-4f6b-bfd4-5df51a19bd3c",
   "metadata": {},
   "source": [
    "### Food for thought: \n",
    "* Can you already see some differences that could present between this data and data collected from a camera on a robot navigating around your house or office?\n",
    "* Do we trust the performance of our model on this data reflects performance on a robot?\n",
    "\n",
    "### Final data preparation\n",
    "\n",
    "ImageNet typically has 1000 classes, whereas our dataset here has only 7 classes. I'm making a dictionary which can convert from our dataset 7 classes back to any related corresponding imagenet classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792c6ae9-4d02-4cc4-b204-c86a726cdea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imagenet_labels import imagenet_classes\n",
    "\n",
    "imagenet_classes = np.array(imagenet_classes)\n",
    "\n",
    "imagenet_idxes = [414, 673, 620, 487, 418, 893, 898]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acda38bd-57bb-4ac2-8140-0d3b4349031d",
   "metadata": {},
   "source": [
    "## 2. Pretrained ResNet50 Model\n",
    "\n",
    "## Initialise the model\n",
    "We will use a pretrained ResNet50, that has been trained on ImageNet already. This is very easy to do in PyTorch -- ```torchvision.models.resnet50``` loads the architecture, and using ```weights=ResNet50_Weights.DEFAULT``` loads the trained parameters for the model after it was trained on ImageNet.\n",
    "\n",
    "You can see all the models built into torchvision [here](https://pytorch.org/vision/stable/models.html#classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205575d6-16bc-4a3f-aa19-7ff91b77de45",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.resnet50(weights=torchvision.models.ResNet50_Weights.DEFAULT)\n",
    "\n",
    "#this line checks if we have a GPU available\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') \n",
    "model = model.to(device)\n",
    "\n",
    "model.eval() # VERY important step, as some layers behave differently during training and testing (i.e. batch norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb49a92-1c5f-43b8-835f-915f34fea1b7",
   "metadata": {},
   "source": [
    "### Food for thought:\n",
    "* Look at the model architecture, in particular, take note of the last layer called 'fc'. It has out_features = 1000 -- what does this mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b56e1c1-63d1-4e2c-be7d-ca4dffffa236",
   "metadata": {},
   "source": [
    "## Performance Metrics\n",
    "\n",
    "The code below is:\n",
    "* Loading images and their ground-truth labels from the test_data we have created\n",
    "* Putting these on the GPU in preparation for going through the model\n",
    "* Passing these through the model to produce 1000 class scores (ImageNet has 1000 classes)\n",
    "* Grabbing the relevant 1000 class scores for our dataset -- 7 \n",
    "\n",
    "### Your turn!\n",
    "Complete the code below to find the performance of the ResNet50 model on our data. In particular, calculate the:\n",
    "* predicted class label from outputs_subset\n",
    "* accuracy (correct/total)\n",
    "* use the imshow() function to understand why our model is sometimes making mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ecfb45-aa26-4095-9788-090d3887fc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in  tqdm.tqdm(test_data):\n",
    "    inputs, label = data\n",
    "    \n",
    "    inputs = inputs.to(device).unsqueeze(0)\n",
    "\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    outputs_subset = outputs[0][imagenet_idxes]\n",
    "    \n",
    "print(f'Model accuracy is {100.*accuracy :.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08904136-bd87-4796-b91e-2470144be838",
   "metadata": {},
   "source": [
    "#### What types of mistakes does the models make?\n",
    "\n",
    "## Building Confusion matrices\n",
    "\n",
    "Using the code above, collect a list of all the predictions and ground-truth labels into lists named 'gt' and 'pred'. You can use this to build a confusion matrix that visualises the types of errors the model is making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e463209-394f-4bc6-957f-5feac3ca9efe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a5311fb-5954-42f5-92b4-19e91d755b1e",
   "metadata": {},
   "source": [
    "We can use [ConfusionMatrixDisplay](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html) from sklearn.metrics to create a confusion matrix.\n",
    "\n",
    "To use the function, we need to pass in: \n",
    "- the GT label for each sample\n",
    "- the predicted label for each sample\n",
    "- (optional) display labels for each label (i.e. a list of class strings)\n",
    "- (optional) normalize over 'true' or 'pred' labels to account for class imbalance\n",
    "\n",
    "Below, let's first test our model over the val dataset and collect the GT label and predicted label for each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9274c6-4ab0-4997-93d9-8918b777ef80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "#run once without normalise and then again with normalise\n",
    "ConfusionMatrixDisplay.from_predictions(gt, pred, display_labels = test_data.classes, xticks_rotation = 'vertical')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f18c3a-a63a-4744-8773-e43cbe6ef9df",
   "metadata": {},
   "source": [
    "## Confidence Calibration Curves\n",
    "\n",
    "As discussed in the lecture, calibration curves are useful for understanding how well the confidence scores predicted by a classification model align with the actual accuracy of the model, which is critical in some applications where not just the label but also the uncertainty of the prediction is important -- like robotics. A well-calibrated model should have its predicted confidence probabilities close to the true probability that the prediction is correct.\n",
    "\n",
    "To do this, we need to collect all our GT labels, predictions, and the confidence associated with each prediction.\n",
    "\n",
    "**Note: This relies on class scores being converted to pseudo-probablities using the [torch.nn.function.softmax() function](https://pytorch.org/docs/stable/generated/torch.nn.functional.softmax.html).**\n",
    "\n",
    "Complete the code below to calculate:\n",
    "* all_gt -- a list of all the ground truth labels\n",
    "* all_pred -- a list of all the predicted labels\n",
    "* all_confidences -- a list of all the softmax scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bb9e4c-4cac-4e00-9056-4f96e5d177f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "703ab422-90cb-41af-8278-90b68dc34c68",
   "metadata": {},
   "source": [
    "Once completing the above code, you can run the next cell to see the confidence calibration curves.\n",
    "\n",
    "#### Consider: Is the model: well-calibrated, over-confident, or under-confident? Is it prone to giving a certain confidence more than others?\n",
    "\n",
    "How would this inform the advice you give someone who wants to use the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355490d4-a502-423a-bbed-e945c4c23faf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#create a variable that holds the confidence intervals we will check on a confidence calibration curve\n",
    "conf_ranges = [[0, 10], [10, 20], [20, 30], [30, 40], [40, 50], [50, 60], [60, 70], [70, 80], [80, 90], [90, 100]] \n",
    "\n",
    "#convert our previously collected lists into numpy arrays so that we can easily manipulate them\n",
    "all_pred_conf = np.array(all_confidences)\n",
    "all_pred_class = np.array(all_pred)\n",
    "all_gt_class = np.array(all_gt)\n",
    "\n",
    "actual_accuracy = []\n",
    "conf_level = []\n",
    "conf_counts = []\n",
    "for conf_int in conf_ranges:\n",
    "    lower = conf_int[0]/100 #convert between 0-1\n",
    "    upper = conf_int[1]/100 #convert between 0-1\n",
    "\n",
    "    #create a mask that will collect predictions in the confidence interval -- it must be above the lower thresh AND below the upper thresh\n",
    "    mask = (all_pred_conf >= lower) & (all_pred_conf < upper)\n",
    "    \n",
    "    #collect all predictions and GT data within the range using the mask\n",
    "    preds = all_pred_class[mask]\n",
    "    gt = all_gt_class[mask]\n",
    "    \n",
    "    #find the accuracy of this bin by checking how many correct/total\n",
    "    correct = np.sum(preds == gt)\n",
    "    total = len(preds)\n",
    "    accuracy = correct/total\n",
    "    actual_accuracy += [accuracy] #save the accuracy for this bin to plot later\n",
    "    conf_level += [(upper + lower)/2] #this is the average confidence level for this confidence interval (not necessarily for the predictions in the bin though), we will use this for plotting later\n",
    "\n",
    "    #how many samples in this bin?\n",
    "    conf_counts += [len(preds)]\n",
    "\n",
    "\n",
    "#Create a figure \n",
    "fig, ax = plt.subplots(2, 1, figsize = (5, 7))\n",
    "ax[0].bar(conf_level, actual_accuracy, width = 0.09)\n",
    "ax[0].plot([0, 1], [0, 1], 'r--') #our well-calibrated line\n",
    "ax[0].set_xlabel('Confidence')\n",
    "ax[0].set_ylabel('Accuracy')\n",
    "ax[0].set_title('Confidence Calibration Curve')\n",
    "\n",
    "ax[1].bar(conf_level, conf_counts, width = 0.09)\n",
    "ax[1].set_xlabel('Confidence')\n",
    "ax[1].set_ylabel('Count')\n",
    "\n",
    "plt.savefig('Confidence_curve.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bdd326-7e92-4d8e-832a-792aa8a0fc26",
   "metadata": {},
   "source": [
    "## Exploring Feature Shift\n",
    "\n",
    "We're going to explore how the performance (accuracy, confusion matrix, and confidence calibration) changes when we test a different dataset -- objectnet_subset. Follow the same process from above, but load in the objectnet_subset folder as the dataset.\n",
    "\n",
    "**In the below cell, load in the objectnet_subset data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db77498-3cf7-4e89-b94d-9f0f93e092a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56bb7628-3146-4f0e-94fc-b69031974d50",
   "metadata": {},
   "source": [
    "**In the below cell, use imshow() to visualise some of the objectnet_subset data.**\n",
    "#### Consider: How does this data look different to the ImageNet data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37fa46d-7670-47a5-a909-64206201e9bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "186389ab-c422-46f2-89d6-b46e62f636ed",
   "metadata": {},
   "source": [
    "**In the below cell, calculate the accuracy on the new objectnet_subset data**\n",
    "#### Consider: How does accuracy change for this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cfb0c5-eac1-4be8-8052-3e05e2c79f10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4638f7ed-5473-4c63-8a18-21bbf1010c90",
   "metadata": {},
   "source": [
    "**In the below cells, build a confusion matrix for the objectnet_subset data**\n",
    "#### Consider: How do the types of errors change for this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4d802c-e058-4deb-a93c-8996ab7f09b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecec93cc-ac10-41ba-8f29-60fe815b6105",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98faa994-5511-46ed-ac99-6ae24b2cf8eb",
   "metadata": {},
   "source": [
    "**In the below cell, build the confidence calibration for the objectnet_subset data.**\n",
    "#### Consider: Has the confidence calibration changed for this dataset?                                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b85501c-bb31-44ed-a753-bb4c132e2589",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913f78e2-164e-4b17-8ce3-d91590c95b00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a84ac6ff-0076-44d5-9f34-86b0592dbf99",
   "metadata": {},
   "source": [
    "## Investigating Class Shift\n",
    "\n",
    "For the last part of this practical, you're going to investigate what happens when we pass images of classes **not** in imagenet class list into the model. You can run the cell below to see the list of imagenet classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ba6e5a-211b-4982-8309-feab4618bbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(imagenet_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065368be-c025-4f5b-bada-489e8165461d",
   "metadata": {},
   "source": [
    "Download images from the internet, or take them on your phone and upload them. You can click the 'upload' symbol under the toolbar and choose an image to upload to JupyterLab. \n",
    "\n",
    "Adapt the code below to visualise the image, predicted class label and confidence associated. Currently it only shows the image and predicted class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f52eedb-454e-4b30-b838-8c11574cc117",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "im_name = 'hamster.jpg'\n",
    "im = Image.open(im_name)\n",
    "\n",
    "inputs = transform(im).unsqueeze(0).to(device)\n",
    "outputs = model(inputs)\n",
    "outputs_subset = outputs[0][imagenet_idxes]\n",
    "\n",
    "predicted = torch.argmax(outputs_subset).cpu().item()\n",
    "class_name = class_idx_to_lbl[predicted]\n",
    "\n",
    "imshow(inputs[0].cpu(), f'{class_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f524f3d-2634-4e1d-a9f9-db2af6fcccb0",
   "metadata": {},
   "source": [
    "#### Time permitting: Try taking photos of the classes in the dataset with your phone and your own version of feature shift, i.e. cluttered images, images at a distance, objects stacked on top of each other, etc. and see how the model responds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ce51db-2d3d-4ca4-b69e-3fd5659b7706",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
