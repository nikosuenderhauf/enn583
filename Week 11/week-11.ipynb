{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bfa0fdc-cca1-4ebe-9c54-f721ffdbf71d",
   "metadata": {},
   "source": [
    "# ENN583 Week 11 - Segmentation\n",
    "\n",
    "This week's practical will explore semantic segmentation models available via PyTorch. You will analyse the predictions and associated confidences of multiple segmentation models, and investigate how combining the predictions of models (i.e. ensembling) can often improve performance.\n",
    "\n",
    "In particular, we'll look at:\n",
    "* a [Fully Convolutional Network](https://arxiv.org/abs/1411.4038) -- as we explored in the lecture\n",
    "* [DeepLabv3](https://pytorch.org/vision/stable/models/deeplabv3.html) -- a more recent, high-performing architecture\n",
    "* [Deep Ensembles](https://arxiv.org/abs/1612.01474) -- which combines these two models!\n",
    "\n",
    "## Step 1: Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb91aa8-0f9b-4530-aeb3-6d3c79e1adc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.io.image import read_image\n",
    "from torchvision.models.segmentation import fcn_resnet50, FCN_ResNet50_Weights, deeplabv3_resnet50, DeepLabV3_ResNet50_Weights\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.colors import LinearSegmentedColormap, ListedColormap\n",
    "import matplotlib\n",
    "\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a23df2b-1fc0-4687-8d52-4b58e5023f73",
   "metadata": {},
   "source": [
    "## Step 2: Load the segmentation models from PyTorch\n",
    "\n",
    "PyTorch has a number of pre-trained models available, including semantic segmentation models trained on Pascal VOC! \n",
    "\n",
    "You can see these [here](https://pytorch.org/vision/stable/models.html#semantic-segmentation).\n",
    "\n",
    "As mentioned, we will compare FCN and DeepLabv3. Compare the 'Mean IoU' of each of these models as listed on the linked Pytorch page -- which model do you expect to perform the best? Also consider the model number of parameters, and GFLOPS -- is there a big difference?\n",
    "\n",
    "Below, we have built off the pytorch example code to visualise how to use a segmentation model. This is very similar to the process that you followed with the object detection models. Note the key steps:\n",
    "1. Initialise the model, load the weights and set into 'eval' mode\n",
    "2. Load the data transforms necessary -- these typically ensure the image is the correct size and is normalized\n",
    "3. Preprocess the data using the data transform -- pretty self-explanatory\n",
    "4. Test the data with the model\n",
    "\n",
    "Below, we're printing two things:\n",
    "* the list of classes that the model was trained on\n",
    "* the size of the input to the model -- the image after it goes through pre-processing\n",
    "* the size of the output (prediction) from the segmentation model\n",
    "\n",
    "What does the output represent? Why does it have different dimensions to the input?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8316cf56-06fa-46fd-8d67-fe4808ffb59f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img = read_image(\"data/dog.jpg\")\n",
    "\n",
    "# Step 1: Initialize model with the best available weights\n",
    "weights = FCN_ResNet50_Weights.DEFAULT\n",
    "model = fcn_resnet50(weights=weights)\n",
    "model.eval()\n",
    "\n",
    "# Step 2: Initialize the inference transforms\n",
    "preprocess = weights.transforms()\n",
    "\n",
    "# Step 3: Apply inference preprocessing transforms\n",
    "batch = preprocess(img).unsqueeze(0)\n",
    "\n",
    "# Step 4: Use the model and visualize the prediction\n",
    "with torch.no_grad():\n",
    "    prediction = model(batch)[\"out\"]\n",
    "    normalized_masks = prediction.softmax(dim=1)\n",
    "\n",
    "print(f\"Input size: {batch.size()}\")\n",
    "print(f\"Output size: {normalized_masks.size()}\")\n",
    "\n",
    "class_list = weights.meta[\"categories\"]\n",
    "class_list[0] = \"background\"\n",
    "print(f\"Classes: {class_list}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9837e3b5-bb77-4790-937d-5acf0cccd38c",
   "metadata": {},
   "source": [
    "## **Your turn!** \n",
    "\n",
    "Adapt the output from the model to create an image representing the prediction from the model. This should be a 2D image, where each pixel is represented as a number between 0 and 20 (representing the predicted class index from the 21 classes of the model).\n",
    "\n",
    "You can pass this image into the ```draw_seg_results``` function, alongside the original image, to visualise the prediction and its corresponding image. \n",
    "\n",
    "Looking at the prediction, has the model segmented the image correctly? Where does it look like there may be errors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b77241b-da58-4fe0-ab7f-c6f3303693d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_seg_results(img, class_preds):\n",
    "    fig, ax = plt.subplots(1, 2, figsize = (12, 4))\n",
    "    \n",
    "    #visualise the input image\n",
    "    img_vis = np.swapaxes(img, 0, 1)\n",
    "    img_vis = np.swapaxes(img_vis, 1, 2)\n",
    "    ax[0].imshow(img_vis)   \n",
    "    \n",
    "    # a function to visualise the segmentation prediction\n",
    "    # you must provide a 2D array (matching width and height of original image), where each element is a number representing the predicted class of that pixel.\n",
    "    cmap = matplotlib.colormaps.get_cmap('tab20')\n",
    "    new_colors = np.concatenate(([[0.2, 0.2, 0.2]], cmap.colors))\n",
    "    cmap = ListedColormap(new_colors)\n",
    "\n",
    "    ax[1].imshow(class_preds, cmap = cmap, vmin = 0, vmax = 20)\n",
    "    \n",
    "    patches = [mpatches.Patch(color=new_colors[i], label=\"{l}\".format(l=class_list[i]) ) for i in range(len(class_list)) ]\n",
    "    fig.legend(handles=patches, bbox_to_anchor=(0.2, 0.05), loc=2, borderaxespad=0. , ncols = 5)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "### Enter your code below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b848b974-c579-4f29-8ee6-ef88b994d466",
   "metadata": {
    "tags": []
   },
   "source": [
    "Observing the segmentation result above, you may notice 2 areas or error:\n",
    "1. The back leg of the dog has been classified as 'cat'.\n",
    "2. The boundary of the dog is classified randomly between the classes (notice the \"rainbow\" colour?).\n",
    "\n",
    "Let's observe the model's confidence in these predictions -- are these confident mistakes? or not?\n",
    "\n",
    "## **Your turn!** \n",
    "\n",
    "Convert the ```normalised_masks``` into an image which relates to the confidence of the predicted class. Add a third subplot into the ```draw_seg_results``` function that can be used to visualise this with ```plt.imshow()```, and observe how the confidence varies across the image. You can use ```plt.colorbar()``` to see the quantitative confidences of each pixel.\n",
    "\n",
    "Compare the changes in confidence to the accuracy of the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d52ccb-5cef-4b6a-bdbe-ea56a243843e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for file_name in glob.glob('data/Road_Anomalies/images/*'):\n",
    "    img = read_image(file_name)\n",
    "   \n",
    "    # Step 1: Initialize model with the best available weights\n",
    "    weights = FCN_ResNet50_Weights.DEFAULT\n",
    "    model = fcn_resnet50(weights=weights)\n",
    "        \n",
    "    model.eval()\n",
    "\n",
    "    # Step 2: Initialize the inference transforms\n",
    "    preprocess = weights.transforms()\n",
    "\n",
    "    # Step 3: Apply inference preprocessing transforms\n",
    "    batch = preprocess(img).unsqueeze(0)\n",
    "\n",
    "    # Step 4: Use the model and visualize the prediction\n",
    "    with torch.no_grad():\n",
    "        prediction = model(batch)[\"out\"]\n",
    "        normalized_masks = prediction.softmax(dim=1)\n",
    "        \n",
    "    #fix the code below...\n",
    "    draw_seg_results(img, ..., ...)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924f8d42-d68d-407f-b64d-99045d118c65",
   "metadata": {},
   "source": [
    "## Compare with another segmentation model!\n",
    "**Your turn:** In the loop above, add in a sub-loop where you also test with the DeepLabv3 ResNet50 model -- everything you need for this is already imported in the first cell, look there for hints on how to refer to the models.\n",
    "\n",
    "Compare the results for the two different models -- is one clearly better? Do they work/fail in different ways?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e3f491-1a83-4608-bb2e-068b28cd11e1",
   "metadata": {},
   "source": [
    "## Two is better than one: Ensembling\n",
    "\n",
    "A common way to improve the performance of models on a task is to ***ensemble*** them -- this means you test an input through multiple models, and then aggregate their predictions to produce an output. There are many benefits of this! \n",
    "\n",
    "1. Often you get more accurate predictions\n",
    "2. Often the confidence is better calibrated -- e.g. more meaningful, high confidence for correct predictions and low for incorrect\n",
    "3. The confidence is also better able to identify novel/anomalous objects that the model hasn't seen before!\n",
    "\n",
    "<img src=\"ensemble.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "**Your turn:** In the loop above, draw the results when you ensemble the predictions of each model. You can do this by averaging their predictions together.\n",
    "\n",
    "Looking at the results, is this helpful for performance? What could be a downside of ensembling?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45996a7-7f0c-4345-9bb1-8b987a6a1b08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
